I"¹<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-cross-validation--dimension-reduction">Lecture: Cross Validation &amp; Dimension Reduction</h2>

<p>2022-01-28</p>

<p>ì§€ê¸ˆê¹Œì§€ ê°„ë‹¨í•œ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ Regressionì— ëŒ€í•œ ì‹¤ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.<br />
ë³¸ íŒŒíŠ¸ëŠ” í›ˆë ¨í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•œ <strong>Cross Validation</strong> ê¸°ë²•ê³¼,<br />
<strong>Dimension Reduction</strong>ì„ í†µí•´ íŠ¹ì • Featureë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì¶”ì¶œí•˜ëŠ” ê¸°ë²•ì— ëŒ€í•œ ìˆ˜ì—…ì„ ì§„í–‰í•˜ì˜€ë‹¤.</p>

<p>ì§€ë‚œ í•™ê¸° ìˆ˜ì—…ë•Œ PCA ê¸°ë²•ì„ ë‹¨ì§€ ê·¸ë¦¼ê³¼ ì•”ê¸°ë¡œë§Œ ì´í•´ë¥¼ í–ˆì—ˆëŠ”ë°,<br />
ì§€ë‚œ ì£¼ì— ë°°ìš´ Linear Algebra ì§€ì‹ì„ í™œìš©í•˜ë‹ˆ ìƒˆë¡œìš´ ì‹œì ìœ¼ë¡œ ì´í•´ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤.</p>

<h2 id="cross-validation">Cross-Validation</h2>

<ul>
  <li>Leave-one-out CV</li>
</ul>

<p>LOOCVëŠ” ex) ì´ 100ê°œì˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´, 99ê°œì˜ Train ë°ì´í„°ì™€ 1ê°œì˜ Test ë°ì´í„°ë¡œ ë‚˜ëˆˆë‹¤.<br />
ê·¸ë¦¬ê³  ì´ëŸ¬í•œ í–‰ìœ„ë¥¼ ì´ 100ë²ˆì— ëŒ€í•´ì„œ ê° ë°ì´í„°ë“¤ì„ ë¬´ì¡°ê±´ í•œ ë²ˆì”© Test ë°ì´í„°ë¡œ ì„¤ì •í•œë‹¤.<br />
ì´ëŠ” ì´ 100ê°œì˜ acc ë° lossì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ë°©ì‹ì´ë‹¤.</p>

<ul>
  <li>K-fold CV
K-foldëŠ” Kë²ˆì”© ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/151482137-40d3786b-feb7-4b26-ba65-73b2c202f541.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 25 03" style="zoom:67%;" /></p>

<h2 id="dimension-reduction">Dimension Reduction</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151482498-716d83e4-01d2-4420-b7e6-2a2557101dd7.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 29 43" style="zoom:67%;" /></p>

<p><strong>Benefits of Dimension Reduction</strong></p>
<ul>
  <li>Less storage: ë°ì´í„°ì˜ ì°¨ì›ì„ ì¤„ì´ë‹ˆ ë‹¹ì—°íˆ ìš©ëŸ‰ì„ ì ê²Œ ë¨¹ìŒ.</li>
  <li>
    <p>Faster computation: 100,000 dim vs. 10 dim vectors ê²°ê³¼ ë‹¹ì—°íˆ 10ê°œ ì°¨ì›ì˜ ê³„ì‚° ì†ë„ê°€ ë¹ ë¦„.</p>
  </li>
  <li>
    <p>Noise removal: ë” ì¢‹ì€ performanceë¥¼ ìœ„í•´ pre-processingë¥¼ ì§„í–‰í•¨.</p>
  </li>
  <li><strong>2D / 3D representation</strong>: Interactive visual exploration.</li>
</ul>

<p><strong>Two Main Techniques</strong></p>

<p>==&gt; <em>Feature = Variable = Dimension</em></p>

<ol>
  <li>Feature <strong>selection</strong></li>
</ol>

<p>Selects a subset of the <strong>original variables</strong> as <u>reduced dimensions.</u></p>

<ul>
  <li>Widely-used criteria.</li>
  <li>Typically combinatorial optimization problems.</li>
  <li><strong>greedy methods</strong> are popular.
    <ul>
      <li><u>Forward</u> selection: <strong>Empty</strong> set -&gt; <strong>Add</strong> one variable at a time.</li>
      <li><u>Backward</u> elimination: <strong>Entire</strong> set -&gt; <strong>Remove</strong> one variable at a time.</li>
    </ul>
  </li>
</ul>

<ol>
  <li>Feature <strong>extraction</strong> (<u>MAIN TOPIC</u>)</li>
</ol>

<p>Each reduced dimension <strong>combines</strong> <u>multiple original dimensions.</u></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151483384-daf50fd9-ef0f-42e7-9b9f-e2ea140ffbab.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 41 06" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151483508-dc94ba59-0c30-461c-ba1f-53a5dfe7ac12.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 42 31" style="zoom:67%;" /></p>

<ul>
  <li>
    <p>Represents each <u>reduced dimension</u> as a <strong>linear combination</strong> of original dimensions.</p>
  </li>
  <li>
    <p>Naturally capable of mapping new data to the same space.</p>
  </li>
</ul>

<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>

<ul>
  <li>
    <p>Finds the axis showing the <u>**largest variation**</u>, and <u>**project**</u> all points into this axis.</p>
  </li>
  <li>
    <p>Reduced dimensions are <strong>orthogonal</strong>.</p>
  </li>
  <li>
    <p>Algorithm: Eigen-decomposition.</p>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/151483999-efe705e8-0bfb-4c69-942d-8621bf12b814.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 48 12" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151484091-7d2aa2bb-d6d9-4a05-a566-3a25279a564a.png" alt="ìŠ¤í¬ë¦°ìƒ· 2022-01-28 ì˜¤í›„ 12 49 21" style="zoom:67%;" /></p>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW14%5D_Multiple_Logistic_Regression.ipynb">HW14 ë§í¬</a></p>
:ET