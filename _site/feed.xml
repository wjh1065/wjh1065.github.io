<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-26T15:12:10+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">while True()</title><subtitle>메모장!.</subtitle><author><name>Lee ChangSeok</name></author><entry><title type="html">goormNLP [4주차 - Linear Regression (3)]</title><link href="http://localhost:4000/goormnlp/Linear_Regression-(3)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Linear Regression (3)]" /><published>2022-01-26T00:00:00+09:00</published><updated>2022-01-26T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Regression%20(3)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Regression-(3)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-logistic-regression">Lecture: Logistic Regression</h2>

<p>2022-01-26</p>

<p>지금까지 주어진 데이터와 가장 잘 맞는 직선을 찾는 <u>Linear Regression</u>을 진행했었다.</p>

<p>이번에는 예측 값이 연속적인 값을 갖지 않는 <strong>Logistic Regression</strong>에 대해서 알아볼 것이다.</p>

<h2 id="classification">Classification</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151110845-ed47db3c-044a-476a-bbd1-a5b7d95ed32c.png" alt="스크린샷 2022-01-26 오후 2 52 41" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151110968-53e6a463-d30c-4d1e-83a0-2567e3715235.png" alt="스크린샷 2022-01-26 오후 2 54 01" style="zoom: 67%;" /></p>

<h2 id="logistic-function">Logistic function</h2>

<p>Logistic regression을 진행하기 위해서는 출력 값을 0과 1의 값으로 맞춰주어야 한다.</p>

<p>이를 위해서 <strong>logistic function</strong> 을 사용했고, Logistic function은 아래와 같다.</p>

<p>$\sigma(z) = \frac{1}{1 + e^{-z}}$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151111302-e49140c7-b8ab-4fac-8cc1-4b3a4c4bafcf.png" alt="image" /></p>

<p>Logistic regression을 진행할 때 입력 데이터를 $x$, 실제 class 값을 $y$, 예측된 출력 값을 $\hat{y}$라고 하면 $x$는 두가지 변환을 거쳐서 $\hat{y}$가 된다.</p>

<p>$z = wx + b$
$\hat{y} = \sigma(z)$</p>

<p>위에 있는 식의 목표는 $\hat{y}$가 실제 $y$와 가장 가깝게 되도록 하는 $w$와 $b$를 찾는 것 이다.</p>

<h2 id="logistic-loss-function">Logistic loss function</h2>

<p>$\sigma(z) = \frac{1}{1 + e^{-z}}$</p>

<p>$\sigma’(z) = \sigma(z) ( 1 - \sigma(z))$</p>

<p>$\frac{\partial{L}}{\partial{\sigma(z)}} = \frac{(y-\sigma(z))}{\sigma(z)(1-\sigma(z))}$</p>

<p>위와 같은 과정을 통해 구한 cost function $L$은 
$L = -y \log(a) + (y-1)\log(1-a)$이 된다.</p>

<p>만약 $y=1$이라면 $L = -\log(a)$만 남게 되며, 그래프로 표현하면 다음과 같다.<br />
실제 class가 1일때 예측 값이 0에 가까워지면 cost function 값이 커지고, 1에 가까워지면 cost function 값이 작아지는 것을 알 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151111896-942a871a-1abd-4a07-a3e2-7bf81a677637.png" alt="image" /></p>

<p>이제 $y=0$이라면 $L = \log(1-a)$ 만 남게 되며, 그래프로 표현하면 다음과 같다.<br />
예측 값이 실제 값이랑 가까워지면 cost function 값이 작아지고 멀어지면 커지게 됨을 알 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151112076-8f7cfbef-4506-40fc-ae3b-1967e4fe8f71.png" alt="image" /></p>

<h2 id="logistic-regression-example">Logistic Regression example</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151112156-4497c2ab-2a8d-4345-91fd-aebb32f6c82a.png" alt="image" /></p>

<p>빨간색 곡선이 Logistic Regression의 모델이다.<br />
기준값을 정한 후, 그것보다 크면 1, 작으면 0으로 분류를 진행하게 된다.<br />
아래 사진은 기준값을 0.5로 설정한 예시이다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151112386-0d305db3-0515-477d-a554-776a5db3dca7.png" alt="image" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="regression" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Linear Regression (2)]</title><link href="http://localhost:4000/goormnlp/Linear_Regression-(2)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Linear Regression (2)]" /><published>2022-01-25T00:00:00+09:00</published><updated>2022-01-25T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Regression%20(2)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Regression-(2)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-linear-regression-with-multiple-variables">Lecture: Linear regression with multiple variables</h2>

<p>2022-01-25</p>

<p>지난번 포스팅에는 하나의 독립 변수 $x$에 대해서 하나의 종속 변수 $y$ 사이의 관계를 알아보는 simple linear regression에 대해서 실습을 하였다.</p>

<p>이번 포스팅에서는 다양한 입력 변수들을 다루는 <strong>multiple linear regression</strong> 에 대해서 알아볼 예정이다.</p>

<h2 id="multiple-features">Multiple features</h2>

<p>기존 하나의 변수를 가진 simple variable은 아래와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905514-fe4ab955-0ae5-4420-bcb3-c90c2f56afe2.png" alt="스크린샷 2022-01-25 오후 12 27 04" style="zoom:67%;" /></p>

<p>그리고 이번에 진행한 여러개의 변수를 가진 multiple variables은 아래와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905595-3587636e-726b-4841-b661-0de24caa8a42.png" alt="스크린샷 2022-01-25 오후 12 27 54" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905641-6d97a16b-2500-4b5f-b31b-bfeb041d7846.png" alt="스크린샷 2022-01-25 오후 12 28 23" style="zoom:67%;" /></p>

<p>스케일링 되지않은 데이터를 이용하여 Regression에 적용한다면 loss가 발산이 될 것이다.</p>

<p>그러므로 모델에 적용 전 데이터들을 <strong>Feature Scaling</strong>을 통해 비슷한 스케일로 조정을 해주어야 한다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905846-2c2f4cc9-5608-43de-a68c-c4dfef77aab3.png" alt="스크린샷 2022-01-25 오후 12 30 14" style="zoom: 50%;" /></p>

<p>Feature Scaling은 $0&lt;=X&lt;=1$, $-1&lt;=X&lt;=1$과 같은 범위로 나오게 되며,</p>

<p>데이터들의 MIN, MAX를 이용한 방법과 데이터들의 MEAN, STD를 이용한 Normalization이 있음.</p>

<p><strong>Polynomial Regression</strong>은 항이 여러 개인 가설 함수로 결과를 예측하는 방법이다.<br />
<u>서로 다른 두 Feature를 하나의 Feature</u>로 만들어서 해당 Feature를 입력으로 2차 이상의 함수를 예측하는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150906733-e65649a3-d975-47a3-b77c-6fe7a7d455de.png" alt="스크린샷 2022-01-25 오후 12 40 06" style="zoom: 50%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150906938-6179f1fc-5d5e-43e5-a27b-c8e9cda2c6d2.png" alt="스크린샷 2022-01-25 오후 12 42 20" style="zoom:50%;" /></p>

<h2 id="example">Example</h2>

<p>실제로 <u>예측을 하고자 할 때 보통 하나 이상의 변수</u>들을 고려해야 한다.<br />
예를 들면 집 값을 예측을 하고자 한다면 <u>집의 크기, 주변의 편의 시설, 위치, 화장실의 개수, 건축 년도 등등 고려해야할 변수들</u>이 많다.</p>

<p>이번 포스팅에서는 다양한 입력 변수 들을 다루는 Multiple linear regression <strong>예시</strong>를 들어 볼 것이다.<br />
사용한 데이터셋: <strong>자동차의 여러 기술적인 사양들을 고려하여 연비를 예측하는 auto miles per gallon(MPG) dataset.</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904247-5cddc436-f796-4068-afc8-93ba00b9068f.png" alt="스크린샷 2022-01-25 오후 12 14 08" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904351-8135aeb4-db27-4afa-8fe3-1b128d58dffa.png" alt="Unknown" /></p>

<p>Accerlation, model_year =&gt; 양의 상관관계.<br />
나머지 =&gt; 음의 상관관계.<br />
위와 같은 상관관계를 통해서 Linear model이 연비를 예측하는데 충분함.</p>

<p>우리가 찾고자하는 모델: \(\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d\).</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904749-02fd2e70-59d5-4b03-9678-5c3abb197007.png" alt="스크린샷 2022-01-25 오후 12 18 48" /></p>

<p>맨 앞에 $x_0 = 1$을 추가했었기 때문에 $\mathbf{X}$의 맨 왼쪽 행렬은 1로 이루어져 있음.</p>

<p>cost function: $L(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2$</p>

<p>gradient descent를 이용하여 weights를 찾은 결과 loss가 무한대를 넘어가서 오류가 발생함.</p>

<p>=&gt; 입력 변수들 중에 특정 값들이 너무 커서 일어난 결과임. <strong>Feature scaling</strong>을 통해 데이터 전처리를 진행하면 해결됨.</p>

<p>$x’ = \frac{x - \min(x)}{\max(x)-\min(x)}$</p>

<hr />

<p>만든 모델이 얼마나 정확한지 알아보기위해, Regression 문제에서 주로 사용되는 두개의 기본 지표를 사용함.</p>

<ol>
  <li>Mean absolute error(MAE)</li>
</ol>

<p>${MAE}(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N \left\vert y^{(i)} - \hat{y}^{(i)}\right\vert$</p>

<ol>
  <li>Root mean squared error(RMSE)</li>
</ol>

<p>${RMSE}(\mathbf{y}, \hat{\mathbf{y}}) =\sqrt{ \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2}$</p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="regression" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Linear Regression (1)]</title><link href="http://localhost:4000/goormnlp/Linear_Regression-(1)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Linear Regression (1)]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Regression%20(1)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Regression-(1)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-linear-regression-with-one-variable">Lecture: Linear regression with one variable</h2>

<p>2022-01-24</p>

<p>Linear regression을 <u>one variable일 때의 Cost function, Gradient descent</u>에 대한 이론 및 실습을 진행하였다.</p>

<p>Linear regression는 Supervised Learning(지도 학습)이라는 큰 범위 내에 포함되어 있고, 한개 이상의 독립 변수 $X$와 종속 변수 $Y$의 선형 관계를 모델링하는 방법론이라 한다.</p>

<p>Regression Problem은 학부 및 대학원에서도 많이 다뤄본 주제이기에 다소 어렵지 않은 내용이였다.</p>

<p>Gradient descent는 머신/딥러닝 알고리즘에서 중요한 이론 중 하나이며 면접에서도 자주 물어보는 내용 중 하나이다.</p>

<h2 id="model-representation">Model representation</h2>

<h3 id="linear-regression">Linear regression</h3>

<p>선형 관계를 모델링한다는 것은 1차로 이루어진 직선을 구하는 것임.</p>

<p>우리의 데이터를 가장 잘 설명하는 최적의 직선을 찾아냄으로써 독립 변수와 종속 변수 사이의 관계를 도출해 내는 과정임.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726237-b44ba52c-0abe-453b-8bbb-f76bf4231181.png" alt="스크린샷 2022-01-24 오후 2 19 18" /></p>

<h3 id="cost-function">Cost function</h3>

<p>우리의 데이터를 가장 잘 설명하는 직선은 우리가 직선을 통해 <u>예측한 값이 실제 데이터의 값과 가장 비슷해야 함</u>. 우리의 모델이 예측한 값은 위에서 알 수 있듯 $f(x_i)$임. 그리고 실제 데이터는 $y$ 입니다. 실제 데이터(위 그림에서 빨간 점) 과 직선 사이의 차이를 줄이는 것이 우리의 목적이며 그것을 바탕으로 cost function을 아래와 같이 정의함.</p>

<p><strong>cost function =</strong> $\frac{1}{N}\sum_{i=1}^n (y_i - f(x_i))^2$</p>

<p>EX)</p>

<p>$f(w) = w^2 + 3w -5$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726811-83cb008d-9d2c-4259-bd8f-745344312b5c.png" alt="스크린샷 2022-01-24 오후 2 25 56" /></p>

<p>f_prime = $2w+3$ ==&gt; $w$ = $-3/2$</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p><img src="https://user-images.githubusercontent.com/67947808/150726913-15732821-cd02-4773-9cd8-7daba1903c20.png" alt="스크린샷 2022-01-24 오후 2 27 13" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fpnum</span> <span class="o">=</span> <span class="n">sympy</span><span class="p">.</span><span class="n">lambdify</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">fprime</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">fpnum</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="mf">10.0</span>   <span class="c1"># starting guess for the min
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">fpnum</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># with 0.01 the step size
</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">1.4999999806458753</span>
</code></pre></div></div>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="regression" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">Pytorch 논문 구현 [UNet (1)]</title><link href="http://localhost:4000/pytorch/UNet-(1)/" rel="alternate" type="text/html" title="Pytorch 논문 구현 [UNet (1)]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/UNet%20(1)</id><content type="html" xml:base="http://localhost:4000/pytorch/UNet-(1)/"><![CDATA[<p>Pytorch Zero to All</p>

<h2 id="unet-1">UNet (1)</h2>

<p>2022-01-24</p>

<p>UNet 모델은 학부생때 딥러닝 주제를 정하고 케라스로 처음으로 구현해본 모델이다. 구현을 한 경험이 있고 논문 또한 정독을 많이 했기에 파이토치로 구현했을때 어려운 점은 없었다. <del>(class 모듈화 공부를 더 해야할 듯..)</del></p>

<p>사용한 데이터: <strong>ISBI 2012 EM segmentation Challenge</strong></p>

<hr />

<script src="https://gist.github.com/wjh1065/03257ee47f96f35175cbb0856747c467.js"></script>

<script src="https://gist.github.com/wjh1065/4141606ea35e1d3d6a64017712d54c65.js"></script>

<script src="https://gist.github.com/wjh1065/eb2b26bfdf28ac0faacca9356a3e1431.js"></script>]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">Pytorch 논문 구현 [MNIST]</title><link href="http://localhost:4000/pytorch/MNIST/" rel="alternate" type="text/html" title="Pytorch 논문 구현 [MNIST]" /><published>2022-01-23T00:00:00+09:00</published><updated>2022-01-23T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/MNIST</id><content type="html" xml:base="http://localhost:4000/pytorch/MNIST/"><![CDATA[<p>Pytorch Zero to All</p>

<h2 id="mnist">MNIST</h2>

<p>2022-01-23</p>

<p>케라스는 파이토치, 텐서플로우에 비해 High Level로써 입문자에게 처음 딥러닝 실습에 적합한 언어라 생각한다. 연구실 생활때, 빠른 결과물을 내야했고.. 파이토치, 텐서플로우를 처음부터 배울 시간이 없었기에 케라스를 이용하여 연구에 대한 결과를 만들어 학회에 발표를 한 경험이 있다.</p>

<p>하지만 인공지능 계열로 취업을 준비했을때 많은 회사들이 Tensorflow보다는 Pytorch 스킬을 요구하였고 나 또한 텐서플로우 기반인 케라스를 다뤄본 적이 있어서 이번 기회에 Pytorch로 전향 할 생각이다.</p>

<p>Pytorch는  상대적으로 Low Level이여서 CUDA 가속화를 할 수 있고 내가 원하는 (정적) 코드로 수정 할 수 있다.</p>

<hr />

<script src="https://gist.github.com/wjh1065/d9e0539abb3a4138a94f487819e6ba82.js"></script>

<script src="https://gist.github.com/wjh1065/57aee1d72476b6e984033027647a9f19.js"></script>

<script src="https://gist.github.com/wjh1065/c60e567eb0d23ed51fe7483cf8340bee.js"></script>]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra Review (2)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-Review/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra Review (2)]" /><published>2022-01-21T00:00:00+09:00</published><updated>2022-01-21T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20Review</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-Review/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h1 id="linear-algebra-review-2">Linear Algebra Review (2)</h1>

<p><img src="https://user-images.githubusercontent.com/67947808/150474459-80a43975-cb2c-41d2-b183-6a0c6962669b.png" alt="스크린샷 2022-01-21 오후 2 06 21" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474464-58ab093b-f72d-4916-aa13-215f9617cd4c.png" alt="스크린샷 2022-01-21 오후 2 06 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474466-ef25c98b-9ecf-4669-97af-ff2cbf5908a4.png" alt="스크린샷 2022-01-21 오후 2 08 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474470-f49a1b5d-f58c-4f65-b440-61bfa99b9b1e.png" alt="스크린샷 2022-01-21 오후 2 08 53" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474472-3bb90e5a-6dcc-4a39-b469-ea8e2eef959d.png" alt="스크린샷 2022-01-21 오후 2 10 02" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474474-fd077512-dd7c-4257-8f1c-46cacf9a9438.png" alt="스크린샷 2022-01-21 오후 2 10 17" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474477-1f2c9840-0dc9-441a-a0bc-88548b91b3ec.png" alt="스크린샷 2022-01-21 오후 2 11 13" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474479-3c70745e-0fd9-437f-8564-b0b2107d799a.png" alt="스크린샷 2022-01-21 오후 2 13 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474481-9d5a1575-6bff-4f9f-acae-c3ee9e801fa7.png" alt="스크린샷 2022-01-21 오후 2 16 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474483-38fcc253-cffd-4dff-a34d-46cec668f103.png" alt="스크린샷 2022-01-21 오후 2 17 35" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474485-85342d9e-6648-4682-9e83-84ded83560c3.png" alt="스크린샷 2022-01-21 오후 2 21 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474486-b35c4b60-b70c-46be-a396-f972a8f08b51.png" alt="스크린샷 2022-01-21 오후 2 24 26" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474490-06a92f22-ef13-4b64-a618-d83612148df0.png" alt="스크린샷 2022-01-21 오후 2 24 34" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474491-98e65ea3-e5a6-4a2a-b352-23348ec3a06b.png" alt="스크린샷 2022-01-21 오후 2 25 06" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474493-668580a3-1c34-4ec5-8883-9eaf79592427.png" alt="스크린샷 2022-01-21 오후 2 25 40" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474495-ee7aa11c-17f3-41bf-9e42-840a769ebf14.png" alt="스크린샷 2022-01-21 오후 2 26 08" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474496-8a8a466c-2181-484a-9863-502d1692d00b.png" alt="스크린샷 2022-01-21 오후 2 27 42" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474498-20fc9f53-c5fe-464a-b9f0-b8c1b87ff941.png" alt="스크린샷 2022-01-21 오후 2 29 41" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474499-48cf00dc-63dc-440d-89e8-2619c5acdaaa.png" alt="스크린샷 2022-01-21 오후 2 31 24" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474500-98a06425-229d-4054-9296-04c60c266ad8.png" alt="스크린샷 2022-01-21 오후 2 32 48" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474502-bb2d9fdd-0a1f-4e01-905b-498afa59115d.png" alt="스크린샷 2022-01-21 오후 2 34 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474504-de00aa03-cbb0-4b3f-87f7-9ed5a986a390.png" alt="스크린샷 2022-01-21 오후 2 36 31" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474505-9f2a4164-052d-40d0-be60-94696e9013af.png" alt="스크린샷 2022-01-21 오후 2 38 07" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474507-8b81b22a-29b8-4e1f-ab64-4c04edd0e991.png" alt="스크린샷 2022-01-21 오후 2 39 38" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474509-8b8491b1-4adb-4b08-9bae-b3e28a4c28a5.png" alt="스크린샷 2022-01-21 오후 2 40 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474512-65922a8a-a373-412b-86cf-41e4b79c8dbc.png" alt="스크린샷 2022-01-21 오후 2 41 19" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474515-3139c7a5-0170-4bf7-855d-868476980c54.png" alt="스크린샷 2022-01-21 오후 2 44 52" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474517-5b9c4d22-9c9a-468c-9733-ba121eb263bb.png" alt="스크린샷 2022-01-21 오후 2 48 56" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474518-379fcb7f-1244-47f0-b4cd-a160780c4254.png" alt="스크린샷 2022-01-21 오후 2 49 29" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474522-ed65b6de-b114-4b3f-ad3d-be9445a212c3.png" alt="스크린샷 2022-01-21 오후 2 51 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474524-28e59954-147c-4a07-95b8-22f5eb62daa6.png" alt="스크린샷 2022-01-21 오후 2 53 20" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474525-3a291a9f-8e76-43b3-aaaf-6ce1c3fffbe4.png" alt="스크린샷 2022-01-21 오후 2 55 22" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474528-19b3e087-688d-4e96-ae3c-cf2d103dc08e.png" alt="스크린샷 2022-01-21 오후 2 57 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474736-8e212bf7-eead-44ee-901c-db4dbdafcc25.png" alt="스크린샷 2022-01-21 오후 3 03 08" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (7)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(7)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (7)]" /><published>2022-01-20T00:00:00+09:00</published><updated>2022-01-20T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(7)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(7)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-9-singular-value-decomposition">Lecture 9: Singular Value Decomposition</h2>

<p>2022-01-20</p>

<p>지금까지 배웠던 선형대수 개념을 이용하여 EVD(고유값 분해)를 진행하였다.</p>

<p>EVD는 정방행렬에 대해서만 적용이 가능하고 이번에 배운 SVD(특이값 분해)는 직사각행렬에 폭넓게 사용이 가능하다.</p>

<p>SVD는 저번 포스팅에 올린 Spectral decomposition을 이용하면 직사각행렬을 고유값을 기저로하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li>SVD (Singular Value Decomposition)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150263278-db454564-2ae9-4778-b748-7d03808872df.png" alt="스크린샷 2022-01-20 오전 11 49 35" style="zoom:80%;" /></p>

<ul>
  <li>U = [u_1, u_2, … , u_k+1, … , u_m]는 <strong>AA<sup>T</sup></strong>를 고유값분해로 직교대각화하여 얻은 <u>m by m</u> orthogonal matrix이며, 특히 [u_1, u_2, … , u_k]를 <strong>left singular vectors</strong>라고 함.</li>
  <li>V = [v_1, v_2, … , v_k+1, … , v_n]는 <strong>A<sup>T</sup>A</strong>를 고유값분해로 직교대각화하여 얻은 <u>n by n</u> orthogonal matrix이며, 특히 [v_1, v_2, … , v_k]를 <strong>right singular vectors</strong>라고 함.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150264809-8f64cf65-9e4e-4cf2-9bb7-cf761077d2ab.png" alt="스크린샷 2022-01-20 오후 12 04 29" style="zoom:100%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264962-5f52d2c6-7f6e-4697-9b1c-5d2ed0810732.png" alt="스크린샷 2022-01-20 오후 12 06 15" /></p>

<blockquote>
  <p>결국 SVD를 계산한다는 것은 AA<sup>T</sup>와 A<sup>T</sup>A의 고유벡터와 고유값을 구하는 것이라는 것을 알 수 있음.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/150265139-e6c5a1f8-a5b5-486d-9de4-088b9ee25beb.png" alt="스크린샷 2022-01-20 오후 12 08 08" style="zoom:85%;" /></p>

<p>* 그림 출처 : https://en.wikipedia.org/wiki/Singular_value_decomposition</p>

<blockquote>
  <p>직교행렬 V<sup>T</sup>에 의해서 원 행렬 M이 회전(방향 전환)을 하게 되며, 시그마에 의해서 크기가 달라졌고(scale 변환), 다시 직교행렬 U에 의해서 V<sup>T</sup>에 의한 회전과는 반대로 회전(방향 전환)을 함.</p>
</blockquote>

<ul>
  <li>고유값분해(EVD)를 통한 대각화의 경우 고유벡터의 방향은 변화가 없고, <strong>크기만 고유값만큼</strong> 변함.</li>
  <li>특이값분해(SVD)는 <strong>U, V<sup>T</sup>에 의해서 M의 방향이 변하고</strong>, <strong>시그마 특이값(singular values)들 만큼의 크기(scale)가 변했음</strong>을 알 수 있음.</li>
</ul>

<p><strong>Reduced Form of SVD</strong></p>

<p>차원 축소할 때는 reduced SVD를 진행함. full SVD 대비 <strong>reduced SVD는 특이값들 중에서 <u>0인 것들을 제외</u></strong>하고 SVD를 함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265797-4b73708c-d44c-419d-887b-545126860bbf.png" alt="스크린샷 2022-01-20 오후 12 14 55" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265924-bd356ea3-049d-49af-a7bd-34a42bb23b80.png" alt="스크린샷 2022-01-20 오후 12 16 18" style="zoom: 70%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265933-8c38c001-696f-4364-894f-0fd57400cdec.png" alt="스크린샷 2022-01-20 오후 12 16 23" style="zoom:90%;" /></p>

<p><strong>example of SVD</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266055-820e3329-1c9f-4743-b2a5-4e1d1f570648.png" alt="스크린샷 2022-01-20 오후 12 17 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266061-27d21602-ba62-45e2-bc26-120bbd6e3835.png" alt="스크린샷 2022-01-20 오후 12 18 04" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266074-ed37966e-e296-419c-82cd-f14259dcc762.png" alt="스크린샷 2022-01-20 오후 12 18 11" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266082-c1dc2e5c-f4c4-464d-b297-cd35e1ba998e.png" alt="스크린샷 2022-01-20 오후 12 18 19" style="zoom:90%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266122-154b4e7a-8678-46e4-b94e-36cab5770150.png" alt="스크린샷 2022-01-20 오후 12 18 51" style="zoom:85%;" /></p>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">R 분석과 프로그래밍</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (6)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(6)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (6)]" /><published>2022-01-19T00:00:00+09:00</published><updated>2022-01-19T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(6)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(6)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-8-advanced-eigendecomposition">Lecture 8: Advanced Eigendecomposition</h2>

<p>2022-01-19</p>

<p>저번 수업 내용을 상기해보면 <em>n</em> x <em>n</em> matrix <em>A</em>가 orthogonally diagonalizable하다면 아래와 같은 식이 성립하게 된다는 것을 알게되었다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150086061-513299b4-d650-4909-9fe5-0ef438e3a839.png" alt="스크린샷 2022-01-19 오후 4 43 15" style="zoom:70%;" /></p>

<p>또한 EVD(고유값 분해)를 Algebraic / Geometric multiplicity 관점으로 바라보았다.</p>

<p>내일 수업에서 보게될 SVD(특이값 분해)를 배우기 전에 오늘은 <strong>Spectral Decomposition</strong>에 대해 공부를 진행하였다.</p>

<p>SVD는 행렬의 스펙트럼 이론을 임의의 직사각행렬에 대해 일반화한 것으로 볼 수 있다. <strong>스펙트럼 이론</strong>을 이용하면 직교 정사각행렬을 고유값을 기저로 하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li><strong>Spectral decomposition</strong></li>
</ul>

<p>An <em>n</em> x <em>n</em> symmetric matrix <em>A</em> has the following properties:</p>

<ol>
  <li><em>A</em> has <em>n</em> <strong>real eigenvalues</strong>, counting multiplicities.</li>
  <li>
    <p>The dimension of the eigenspace for each eigen value λ <strong>equals</strong> the multiplicity of λ as a root of the <em>characteristic equation</em>.
 <strong>The dimension of the eigenspace</strong> ==&gt; Geometric multiplicity.
 <strong>The multiplicity of λ</strong>                          ==&gt; Algebraic multiplicity.</p>
  </li>
  <li>The eigenspaces are mutually orthogonal.</li>
  <li><strong>A</strong> is orthogonally diagonalizable.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/67947808/150087635-792eb8c9-d6b4-479b-99ed-4e2a2f51102f.png" alt="스크린샷 2022-01-19 오후 4 54 24" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150087662-fddc9e8a-823b-40f6-9a85-9472d84ff61e.png" alt="스크린샷 2022-01-19 오후 4 54 37" style="zoom:67%;" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (5)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(5)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (5)]" /><published>2022-01-18T00:00:00+09:00</published><updated>2022-01-18T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(5)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(5)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-8-advanced-eigendecomposition">Lecture 8: Advanced Eigendecomposition</h2>

<p>2022-01-18</p>

<p>저번 포스팅에 고유값(eigenvalue), 고유벡터(eigenvector), 대각행렬(diagonal matrix) ,대각화(diagonalization)를 활용하여 n차 정방행렬의 p제곱을 구하였고, 이번에는 EVD(Eigenvalue-eigenvector Decomposition)에 대한 수업을 진행하였다. EVD는 <u>n by n 정방행렬</u>에 대해서만 적용이 가능하고 <strong>Markov process 과정</strong>을 적용하여 계산하였다.</p>

<ul>
  <li>
    <p>Symmetric Matrix
  If Matrix A is <em>symmetric</em>, then any two eigenvectors from different eigenspaces are <strong>orthogonal</strong>.</p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149876597-4bdf201b-d425-4bbe-9d8c-2ad4608f13d7.png" alt="스크린샷 2022-01-18 오후 2 31 03" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149876648-6847f861-6f2c-4006-8fef-291207988bfe.png" alt="스크린샷 2022-01-18 오후 2 31 28" /></p>

    <blockquote>
      <p>An n x n matrix A is said to be <strong>orthogonally diagonalizable.</strong></p>
    </blockquote>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149876771-ad63c63f-a543-403f-b8c0-8522c4f2e7fb.png" alt="스크린샷 2022-01-18 오후 2 32 38" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149876926-3a90e85d-9768-4f2a-bc69-3d0ac1c2e906.png" alt="스크린샷 2022-01-18 오후 2 34 16" style="zoom:80%;" /></p>

<ul>
  <li>
    <p>ex) <strong>Markov Process</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877164-554aa58c-041c-4370-914d-d118f2978a3c.png" alt="스크린샷 2022-01-18 오후 2 36 26" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877215-a28c5864-2c8f-4fcb-8605-1c053115dab0.png" alt="스크린샷 2022-01-18 오후 2 37 12" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877234-feab56b3-3929-409f-9d5b-0dd7d056bf39.png" alt="스크린샷 2022-01-18 오후 2 37 24" /></p>

    <p><strong>step 1) eigenvalue 구하기</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877300-967da64f-0a5a-47d8-91f3-36b823ff80fd.png" alt="스크린샷 2022-01-18 오후 2 38 17" style="zoom:80%;" /></p>

    <p><strong>step 2) eigenvector 구하기</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877565-13cadd2b-7232-4993-86e1-c7b2f54eda45.png" alt="image" style="zoom:90%;" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877584-db5a5d3c-edc0-451a-9d89-7d4a1f8b7cc7.png" alt="image" style="zoom:80%;" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877611-6abc623a-cd5e-4137-aa59-e12161f5f2b9.png" alt="스크린샷 2022-01-18 오후 2 41 30" style="zoom:85%;" /></p>

    <p><strong>step 3) RESULT</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877701-11c881f4-fe47-4dd0-90e3-0cb2cdf3cd35.png" alt="스크린샷 2022-01-18 오후 2 42 34" style="zoom:80%;" /></p>
  </li>
</ul>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">R 분석과 프로그래밍</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (4)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(4)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (4)]" /><published>2022-01-17T00:00:00+09:00</published><updated>2022-01-17T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(4)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(4)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-7-eigendecomposition">Lecture 7: Eigendecomposition</h2>

<p>2022-01-17</p>

<p>고유값(eigenvalue), 고유벡터(eigenvector)에 대한 <u>정의</u>를 이해하고 <u>값</u>들을 구하였다.</p>

<p>그 후 대각행렬(diagonal matrix) ,대각화(diagonalization)를 활용하여 n차 정방행렬의 <u>p제곱</u>을 구하였다.</p>

<blockquote>
  <p>정방행렬 A에 대하여 Ax = λx (상수 λ) 가 성립하는 0이 아닌 벡터 x가 존재할 때 
상수 λ 를 행렬 A의 고유값 (eigenvalue), x 를 이에 대응하는 고유벡터 (eigenvector) 라고 함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149726517-25f8e133-f092-4c4a-88b4-566223f62550.png" alt="스크린샷 2022-01-17 오후 4 32 48" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149726646-9536a72f-7083-48a1-a14e-b672435f6aa2.png" alt="스크린샷 2022-01-17 오후 4 33 56" style="zoom:85%;" /></p>

<blockquote>
  <p>행렬의 곱의 결과가 원래 벡터와 <strong>“방향”</strong>은 <u>같고</u>, <strong>“배율”</strong>만 상수 λ 만큼만 비례해서 <u>변한다</u>.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727000-43b20905-7c1e-457e-aba0-1ad5aece8ac6.png" alt="스크린샷 2022-01-17 오후 4 36 45" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727034-26906718-5976-475a-88c4-45c51ffb15a6.png" alt="스크린샷 2022-01-17 오후 4 37 01" style="zoom:80%;" /></p>

<blockquote>
  <p>eigenvector의 방향은 똑같고(same direction), 크기만 eigenvalue만큼씩 배수가 됨.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727266-f8a7b2a8-2258-4f5b-af09-cb22033c7c1b.png" alt="스크린샷 2022-01-17 오후 4 38 58" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727382-7c88c62e-3b87-4ad8-ab12-a85539bfc89a.png" alt="스크린샷 2022-01-17 오후 4 39 55" style="zoom:80%;" /></p>

<blockquote>
  <p>고유값과 고유벡터를 구하는 순서는, <strong>먼저 고유값을 구하고나서</strong>, 나중에 Gauss 소거법 사용하여 고유값에 대응하는 고유벡터를 구함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727691-e03462e3-3025-4faf-b416-c16ac7e30e91.png" alt="스크린샷 2022-01-17 오후 4 42 15" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727710-fa7fbac0-94dc-43f3-b90b-50467afe0359.png" alt="스크린샷 2022-01-17 오후 4 42 22" style="zoom:80%;" /></p>

<hr />

<p><img src="https://user-images.githubusercontent.com/67947808/149728001-04c1ded9-7189-4e12-9dfc-3fbf15177fd6.png" alt="스크린샷 2022-01-17 오후 4 44 31" style="zoom:80%;" /></p>

<blockquote>
  <p>대각성분을 제외한 모든 성분이 0인 행렬을 대각행렬(diagonal matrix), 적절한 기저변환을 통하여 주어진 행렬을 대각행렬로 변환하는 것을 대각화(diagonolization)이라 함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149728224-6ea7600c-b6f5-4378-b748-63fe41491d6d.png" alt="스크린샷 2022-01-17 오후 4 46 17" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149728290-9c6d48f0-cb73-4fea-af30-80ce9e8d5b95.png" alt="스크린샷 2022-01-17 오후 4 46 47" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149728322-120eac70-9b9d-42b4-9322-dbd78b6968b5.png" alt="스크린샷 2022-01-17 오후 4 46 59" style="zoom:80%;" /></p>

<blockquote>
  <p><strong>고유값과 고유벡터를 이용한 n차 정방행렬의 p제곱의 정리를 이용하면 p가 매우 크더라도 연산량을 줄여서 쉽게 p제곱을 구할 수 있음</strong>.</p>
</blockquote>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">출처 주소</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry></feed>