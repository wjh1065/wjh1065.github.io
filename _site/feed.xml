<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-24T18:02:08+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">while True()</title><subtitle>메모장!.</subtitle><author><name>Lee ChangSeok</name></author><entry><title type="html">goormNLP [4주차 - Linear Regression (1)]</title><link href="http://localhost:4000/goormnlp/Linear_Regression-(1)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Linear Regression (1)]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Regression%20(1)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Regression-(1)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-linear-regression-with-one-variable">Lecture: Linear regression with one variable</h2>

<p>2022-01-24</p>

<p>Linear regression을 <u>one variable일 때의 Cost function, Gradient descent</u>에 대한 이론 및 실습을 진행하였다.</p>

<p>Linear regression는 Supervised Learning(지도 학습)이라는 큰 범위 내에 포함되어 있고, 한개 이상의 독립 변수 $X$와 종속 변수 $Y$의 선형 관계를 모델링하는 방법론이라 한다.</p>

<p>Regression Problem은 학부 및 대학원에서도 많이 다뤄본 주제이기에 다소 어렵지 않은 내용이였다.</p>

<p>Gradient descent는 머신/딥러닝 알고리즘에서 중요한 이론 중 하나이며 면접에서도 자주 물어보는 내용 중 하나이다.</p>

<h2 id="model-representation">Model representation</h2>

<h3 id="linear-regression">Linear regression</h3>

<p>선형 관계를 모델링한다는 것은 1차로 이루어진 직선을 구하는 것임.</p>

<p>우리의 데이터를 가장 잘 설명하는 최적의 직선을 찾아냄으로써 독립 변수와 종속 변수 사이의 관계를 도출해 내는 과정임.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726237-b44ba52c-0abe-453b-8bbb-f76bf4231181.png" alt="스크린샷 2022-01-24 오후 2 19 18" /></p>

<h3 id="cost-function">Cost function</h3>

<p>우리의 데이터를 가장 잘 설명하는 직선은 우리가 직선을 통해 <u>예측한 값이 실제 데이터의 값과 가장 비슷해야 함</u>. 우리의 모델이 예측한 값은 위에서 알 수 있듯 $f(x_i)$임. 그리고 실제 데이터는 $y$ 입니다. 실제 데이터(위 그림에서 빨간 점) 과 직선 사이의 차이를 줄이는 것이 우리의 목적이며 그것을 바탕으로 cost function을 아래와 같이 정의함.</p>

<p><strong>cost function =</strong> $\frac{1}{N}\sum_{i=1}^n (y_i - f(x_i))^2$</p>

<p>EX)</p>

<p>$f(w) = w^2 + 3w -5$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726811-83cb008d-9d2c-4259-bd8f-745344312b5c.png" alt="스크린샷 2022-01-24 오후 2 25 56" /></p>

<p>f_prime = $2w+3$ ==&gt; $w$ = $-3/2$</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p><img src="https://user-images.githubusercontent.com/67947808/150726913-15732821-cd02-4773-9cd8-7daba1903c20.png" alt="스크린샷 2022-01-24 오후 2 27 13" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fpnum</span> <span class="o">=</span> <span class="n">sympy</span><span class="p">.</span><span class="n">lambdify</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">fprime</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">fpnum</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="mf">10.0</span>   <span class="c1"># starting guess for the min
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">fpnum</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># with 0.01 the step size
</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">1.4999999806458753</span>
</code></pre></div></div>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="regression" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">Pytorch 논문 구현 [UNet (1)]</title><link href="http://localhost:4000/pytorch/UNet-(1)/" rel="alternate" type="text/html" title="Pytorch 논문 구현 [UNet (1)]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/UNet%20(1)</id><content type="html" xml:base="http://localhost:4000/pytorch/UNet-(1)/"><![CDATA[<p>Pytorch Zero to All</p>

<h2 id="unet-1">UNet (1)</h2>

<p>2022-01-24</p>

<p>UNet 모델은 학부생때 딥러닝 주제를 정하고 케라스로 처음으로 구현해본 모델이다. 구현을 한 경험이 있고 논문 또한 정독을 많이 했기에 파이토치로 구현했을때 어려운 점은 없었다. <del>(class 모듈화 공부를 더 해야할 듯..)</del></p>

<p>사용한 데이터: <strong>ISBI 2012 EM segmentation Challenge</strong></p>

<hr />

<script src="https://gist.github.com/wjh1065/03257ee47f96f35175cbb0856747c467.js"></script>

<script src="https://gist.github.com/wjh1065/4141606ea35e1d3d6a64017712d54c65.js"></script>

<script src="https://gist.github.com/wjh1065/eb2b26bfdf28ac0faacca9356a3e1431.js"></script>]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">Pytorch 논문 구현 [MNIST]</title><link href="http://localhost:4000/pytorch/MNIST/" rel="alternate" type="text/html" title="Pytorch 논문 구현 [MNIST]" /><published>2022-01-23T00:00:00+09:00</published><updated>2022-01-23T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/MNIST</id><content type="html" xml:base="http://localhost:4000/pytorch/MNIST/"><![CDATA[<p>Pytorch Zero to All</p>

<h2 id="mnist">MNIST</h2>

<p>2022-01-23</p>

<p>케라스는 파이토치, 텐서플로우에 비해 High Level로써 입문자에게 처음 딥러닝 실습에 적합한 언어라 생각한다. 연구실 생활때, 빠른 결과물을 내야했고.. 파이토치, 텐서플로우를 처음부터 배울 시간이 없었기에 케라스를 이용하여 연구에 대한 결과를 만들어 학회에 발표를 한 경험이 있다.</p>

<p>하지만 인공지능 계열로 취업을 준비했을때 많은 회사들이 Tensorflow보다는 Pytorch 스킬을 요구하였고 나 또한 텐서플로우 기반인 케라스를 다뤄본 적이 있어서 이번 기회에 Pytorch로 전향 할 생각이다.</p>

<p>Pytorch는  상대적으로 Low Level이여서 CUDA 가속화를 할 수 있고 내가 원하는 (정적) 코드로 수정 할 수 있다.</p>

<hr />

<script src="https://gist.github.com/wjh1065/d9e0539abb3a4138a94f487819e6ba82.js"></script>

<script src="https://gist.github.com/wjh1065/57aee1d72476b6e984033027647a9f19.js"></script>

<script src="https://gist.github.com/wjh1065/c60e567eb0d23ed51fe7483cf8340bee.js"></script>]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra Review (2)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-Review/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra Review (2)]" /><published>2022-01-21T00:00:00+09:00</published><updated>2022-01-21T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20Review</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-Review/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h1 id="linear-algebra-review-2">Linear Algebra Review (2)</h1>

<p><img src="https://user-images.githubusercontent.com/67947808/150474459-80a43975-cb2c-41d2-b183-6a0c6962669b.png" alt="스크린샷 2022-01-21 오후 2 06 21" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474464-58ab093b-f72d-4916-aa13-215f9617cd4c.png" alt="스크린샷 2022-01-21 오후 2 06 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474466-ef25c98b-9ecf-4669-97af-ff2cbf5908a4.png" alt="스크린샷 2022-01-21 오후 2 08 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474470-f49a1b5d-f58c-4f65-b440-61bfa99b9b1e.png" alt="스크린샷 2022-01-21 오후 2 08 53" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474472-3bb90e5a-6dcc-4a39-b469-ea8e2eef959d.png" alt="스크린샷 2022-01-21 오후 2 10 02" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474474-fd077512-dd7c-4257-8f1c-46cacf9a9438.png" alt="스크린샷 2022-01-21 오후 2 10 17" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474477-1f2c9840-0dc9-441a-a0bc-88548b91b3ec.png" alt="스크린샷 2022-01-21 오후 2 11 13" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474479-3c70745e-0fd9-437f-8564-b0b2107d799a.png" alt="스크린샷 2022-01-21 오후 2 13 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474481-9d5a1575-6bff-4f9f-acae-c3ee9e801fa7.png" alt="스크린샷 2022-01-21 오후 2 16 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474483-38fcc253-cffd-4dff-a34d-46cec668f103.png" alt="스크린샷 2022-01-21 오후 2 17 35" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474485-85342d9e-6648-4682-9e83-84ded83560c3.png" alt="스크린샷 2022-01-21 오후 2 21 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474486-b35c4b60-b70c-46be-a396-f972a8f08b51.png" alt="스크린샷 2022-01-21 오후 2 24 26" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474490-06a92f22-ef13-4b64-a618-d83612148df0.png" alt="스크린샷 2022-01-21 오후 2 24 34" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474491-98e65ea3-e5a6-4a2a-b352-23348ec3a06b.png" alt="스크린샷 2022-01-21 오후 2 25 06" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474493-668580a3-1c34-4ec5-8883-9eaf79592427.png" alt="스크린샷 2022-01-21 오후 2 25 40" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474495-ee7aa11c-17f3-41bf-9e42-840a769ebf14.png" alt="스크린샷 2022-01-21 오후 2 26 08" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474496-8a8a466c-2181-484a-9863-502d1692d00b.png" alt="스크린샷 2022-01-21 오후 2 27 42" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474498-20fc9f53-c5fe-464a-b9f0-b8c1b87ff941.png" alt="스크린샷 2022-01-21 오후 2 29 41" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474499-48cf00dc-63dc-440d-89e8-2619c5acdaaa.png" alt="스크린샷 2022-01-21 오후 2 31 24" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474500-98a06425-229d-4054-9296-04c60c266ad8.png" alt="스크린샷 2022-01-21 오후 2 32 48" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474502-bb2d9fdd-0a1f-4e01-905b-498afa59115d.png" alt="스크린샷 2022-01-21 오후 2 34 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474504-de00aa03-cbb0-4b3f-87f7-9ed5a986a390.png" alt="스크린샷 2022-01-21 오후 2 36 31" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474505-9f2a4164-052d-40d0-be60-94696e9013af.png" alt="스크린샷 2022-01-21 오후 2 38 07" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474507-8b81b22a-29b8-4e1f-ab64-4c04edd0e991.png" alt="스크린샷 2022-01-21 오후 2 39 38" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474509-8b8491b1-4adb-4b08-9bae-b3e28a4c28a5.png" alt="스크린샷 2022-01-21 오후 2 40 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474512-65922a8a-a373-412b-86cf-41e4b79c8dbc.png" alt="스크린샷 2022-01-21 오후 2 41 19" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474515-3139c7a5-0170-4bf7-855d-868476980c54.png" alt="스크린샷 2022-01-21 오후 2 44 52" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474517-5b9c4d22-9c9a-468c-9733-ba121eb263bb.png" alt="스크린샷 2022-01-21 오후 2 48 56" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474518-379fcb7f-1244-47f0-b4cd-a160780c4254.png" alt="스크린샷 2022-01-21 오후 2 49 29" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474522-ed65b6de-b114-4b3f-ad3d-be9445a212c3.png" alt="스크린샷 2022-01-21 오후 2 51 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474524-28e59954-147c-4a07-95b8-22f5eb62daa6.png" alt="스크린샷 2022-01-21 오후 2 53 20" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474525-3a291a9f-8e76-43b3-aaaf-6ce1c3fffbe4.png" alt="스크린샷 2022-01-21 오후 2 55 22" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474528-19b3e087-688d-4e96-ae3c-cf2d103dc08e.png" alt="스크린샷 2022-01-21 오후 2 57 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474736-8e212bf7-eead-44ee-901c-db4dbdafcc25.png" alt="스크린샷 2022-01-21 오후 3 03 08" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (7)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(7)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (7)]" /><published>2022-01-20T00:00:00+09:00</published><updated>2022-01-20T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(7)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(7)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-9-singular-value-decomposition">Lecture 9: Singular Value Decomposition</h2>

<p>2022-01-20</p>

<p>지금까지 배웠던 선형대수 개념을 이용하여 EVD(고유값 분해)를 진행하였다.</p>

<p>EVD는 정방행렬에 대해서만 적용이 가능하고 이번에 배운 SVD(특이값 분해)는 직사각행렬에 폭넓게 사용이 가능하다.</p>

<p>SVD는 저번 포스팅에 올린 Spectral decomposition을 이용하면 직사각행렬을 고유값을 기저로하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li>SVD (Singular Value Decomposition)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150263278-db454564-2ae9-4778-b748-7d03808872df.png" alt="스크린샷 2022-01-20 오전 11 49 35" style="zoom:80%;" /></p>

<ul>
  <li>U = [u_1, u_2, … , u_k+1, … , u_m]는 <strong>AA<sup>T</sup></strong>를 고유값분해로 직교대각화하여 얻은 <u>m by m</u> orthogonal matrix이며, 특히 [u_1, u_2, … , u_k]를 <strong>left singular vectors</strong>라고 함.</li>
  <li>V = [v_1, v_2, … , v_k+1, … , v_n]는 <strong>A<sup>T</sup>A</strong>를 고유값분해로 직교대각화하여 얻은 <u>n by n</u> orthogonal matrix이며, 특히 [v_1, v_2, … , v_k]를 <strong>right singular vectors</strong>라고 함.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150264809-8f64cf65-9e4e-4cf2-9bb7-cf761077d2ab.png" alt="스크린샷 2022-01-20 오후 12 04 29" style="zoom:100%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264962-5f52d2c6-7f6e-4697-9b1c-5d2ed0810732.png" alt="스크린샷 2022-01-20 오후 12 06 15" /></p>

<blockquote>
  <p>결국 SVD를 계산한다는 것은 AA<sup>T</sup>와 A<sup>T</sup>A의 고유벡터와 고유값을 구하는 것이라는 것을 알 수 있음.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/150265139-e6c5a1f8-a5b5-486d-9de4-088b9ee25beb.png" alt="스크린샷 2022-01-20 오후 12 08 08" style="zoom:85%;" /></p>

<p>* 그림 출처 : https://en.wikipedia.org/wiki/Singular_value_decomposition</p>

<blockquote>
  <p>직교행렬 V<sup>T</sup>에 의해서 원 행렬 M이 회전(방향 전환)을 하게 되며, 시그마에 의해서 크기가 달라졌고(scale 변환), 다시 직교행렬 U에 의해서 V<sup>T</sup>에 의한 회전과는 반대로 회전(방향 전환)을 함.</p>
</blockquote>

<ul>
  <li>고유값분해(EVD)를 통한 대각화의 경우 고유벡터의 방향은 변화가 없고, <strong>크기만 고유값만큼</strong> 변함.</li>
  <li>특이값분해(SVD)는 <strong>U, V<sup>T</sup>에 의해서 M의 방향이 변하고</strong>, <strong>시그마 특이값(singular values)들 만큼의 크기(scale)가 변했음</strong>을 알 수 있음.</li>
</ul>

<p><strong>Reduced Form of SVD</strong></p>

<p>차원 축소할 때는 reduced SVD를 진행함. full SVD 대비 <strong>reduced SVD는 특이값들 중에서 <u>0인 것들을 제외</u></strong>하고 SVD를 함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265797-4b73708c-d44c-419d-887b-545126860bbf.png" alt="스크린샷 2022-01-20 오후 12 14 55" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265924-bd356ea3-049d-49af-a7bd-34a42bb23b80.png" alt="스크린샷 2022-01-20 오후 12 16 18" style="zoom: 70%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265933-8c38c001-696f-4364-894f-0fd57400cdec.png" alt="스크린샷 2022-01-20 오후 12 16 23" style="zoom:90%;" /></p>

<p><strong>example of SVD</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266055-820e3329-1c9f-4743-b2a5-4e1d1f570648.png" alt="스크린샷 2022-01-20 오후 12 17 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266061-27d21602-ba62-45e2-bc26-120bbd6e3835.png" alt="스크린샷 2022-01-20 오후 12 18 04" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266074-ed37966e-e296-419c-82cd-f14259dcc762.png" alt="스크린샷 2022-01-20 오후 12 18 11" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266082-c1dc2e5c-f4c4-464d-b297-cd35e1ba998e.png" alt="스크린샷 2022-01-20 오후 12 18 19" style="zoom:90%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266122-154b4e7a-8678-46e4-b94e-36cab5770150.png" alt="스크린샷 2022-01-20 오후 12 18 51" style="zoom:85%;" /></p>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">R 분석과 프로그래밍</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (6)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(6)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (6)]" /><published>2022-01-19T00:00:00+09:00</published><updated>2022-01-19T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(6)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(6)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-8-advanced-eigendecomposition">Lecture 8: Advanced Eigendecomposition</h2>

<p>2022-01-19</p>

<p>저번 수업 내용을 상기해보면 <em>n</em> x <em>n</em> matrix <em>A</em>가 orthogonally diagonalizable하다면 아래와 같은 식이 성립하게 된다는 것을 알게되었다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150086061-513299b4-d650-4909-9fe5-0ef438e3a839.png" alt="스크린샷 2022-01-19 오후 4 43 15" style="zoom:70%;" /></p>

<p>또한 EVD(고유값 분해)를 Algebraic / Geometric multiplicity 관점으로 바라보았다.</p>

<p>내일 수업에서 보게될 SVD(특이값 분해)를 배우기 전에 오늘은 <strong>Spectral Decomposition</strong>에 대해 공부를 진행하였다.</p>

<p>SVD는 행렬의 스펙트럼 이론을 임의의 직사각행렬에 대해 일반화한 것으로 볼 수 있다. <strong>스펙트럼 이론</strong>을 이용하면 직교 정사각행렬을 고유값을 기저로 하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li><strong>Spectral decomposition</strong></li>
</ul>

<p>An <em>n</em> x <em>n</em> symmetric matrix <em>A</em> has the following properties:</p>

<ol>
  <li><em>A</em> has <em>n</em> <strong>real eigenvalues</strong>, counting multiplicities.</li>
  <li>
    <p>The dimension of the eigenspace for each eigen value λ <strong>equals</strong> the multiplicity of λ as a root of the <em>characteristic equation</em>.
 <strong>The dimension of the eigenspace</strong> ==&gt; Geometric multiplicity.
 <strong>The multiplicity of λ</strong>                          ==&gt; Algebraic multiplicity.</p>
  </li>
  <li>The eigenspaces are mutually orthogonal.</li>
  <li><strong>A</strong> is orthogonally diagonalizable.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/67947808/150087635-792eb8c9-d6b4-479b-99ed-4e2a2f51102f.png" alt="스크린샷 2022-01-19 오후 4 54 24" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150087662-fddc9e8a-823b-40f6-9a85-9472d84ff61e.png" alt="스크린샷 2022-01-19 오후 4 54 37" style="zoom:67%;" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (5)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(5)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (5)]" /><published>2022-01-18T00:00:00+09:00</published><updated>2022-01-18T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(5)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(5)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-8-advanced-eigendecomposition">Lecture 8: Advanced Eigendecomposition</h2>

<p>2022-01-18</p>

<p>저번 포스팅에 고유값(eigenvalue), 고유벡터(eigenvector), 대각행렬(diagonal matrix) ,대각화(diagonalization)를 활용하여 n차 정방행렬의 p제곱을 구하였고, 이번에는 EVD(Eigenvalue-eigenvector Decomposition)에 대한 수업을 진행하였다. EVD는 <u>n by n 정방행렬</u>에 대해서만 적용이 가능하고 <strong>Markov process 과정</strong>을 적용하여 계산하였다.</p>

<ul>
  <li>
    <p>Symmetric Matrix
  If Matrix A is <em>symmetric</em>, then any two eigenvectors from different eigenspaces are <strong>orthogonal</strong>.</p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149876597-4bdf201b-d425-4bbe-9d8c-2ad4608f13d7.png" alt="스크린샷 2022-01-18 오후 2 31 03" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149876648-6847f861-6f2c-4006-8fef-291207988bfe.png" alt="스크린샷 2022-01-18 오후 2 31 28" /></p>

    <blockquote>
      <p>An n x n matrix A is said to be <strong>orthogonally diagonalizable.</strong></p>
    </blockquote>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149876771-ad63c63f-a543-403f-b8c0-8522c4f2e7fb.png" alt="스크린샷 2022-01-18 오후 2 32 38" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149876926-3a90e85d-9768-4f2a-bc69-3d0ac1c2e906.png" alt="스크린샷 2022-01-18 오후 2 34 16" style="zoom:80%;" /></p>

<ul>
  <li>
    <p>ex) <strong>Markov Process</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877164-554aa58c-041c-4370-914d-d118f2978a3c.png" alt="스크린샷 2022-01-18 오후 2 36 26" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877215-a28c5864-2c8f-4fcb-8605-1c053115dab0.png" alt="스크린샷 2022-01-18 오후 2 37 12" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877234-feab56b3-3929-409f-9d5b-0dd7d056bf39.png" alt="스크린샷 2022-01-18 오후 2 37 24" /></p>

    <p><strong>step 1) eigenvalue 구하기</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877300-967da64f-0a5a-47d8-91f3-36b823ff80fd.png" alt="스크린샷 2022-01-18 오후 2 38 17" style="zoom:80%;" /></p>

    <p><strong>step 2) eigenvector 구하기</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877565-13cadd2b-7232-4993-86e1-c7b2f54eda45.png" alt="image" style="zoom:90%;" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877584-db5a5d3c-edc0-451a-9d89-7d4a1f8b7cc7.png" alt="image" style="zoom:80%;" /></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877611-6abc623a-cd5e-4137-aa59-e12161f5f2b9.png" alt="스크린샷 2022-01-18 오후 2 41 30" style="zoom:85%;" /></p>

    <p><strong>step 3) RESULT</strong></p>

    <p><img src="https://user-images.githubusercontent.com/67947808/149877701-11c881f4-fe47-4dd0-90e3-0cb2cdf3cd35.png" alt="스크린샷 2022-01-18 오후 2 42 34" style="zoom:80%;" /></p>
  </li>
</ul>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">R 분석과 프로그래밍</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (4)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(4)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (4)]" /><published>2022-01-17T00:00:00+09:00</published><updated>2022-01-17T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(4)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(4)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-7-eigendecomposition">Lecture 7: Eigendecomposition</h2>

<p>2022-01-17</p>

<p>고유값(eigenvalue), 고유벡터(eigenvector)에 대한 <u>정의</u>를 이해하고 <u>값</u>들을 구하였다.</p>

<p>그 후 대각행렬(diagonal matrix) ,대각화(diagonalization)를 활용하여 n차 정방행렬의 <u>p제곱</u>을 구하였다.</p>

<blockquote>
  <p>정방행렬 A에 대하여 Ax = λx (상수 λ) 가 성립하는 0이 아닌 벡터 x가 존재할 때 
상수 λ 를 행렬 A의 고유값 (eigenvalue), x 를 이에 대응하는 고유벡터 (eigenvector) 라고 함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149726517-25f8e133-f092-4c4a-88b4-566223f62550.png" alt="스크린샷 2022-01-17 오후 4 32 48" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149726646-9536a72f-7083-48a1-a14e-b672435f6aa2.png" alt="스크린샷 2022-01-17 오후 4 33 56" style="zoom:85%;" /></p>

<blockquote>
  <p>행렬의 곱의 결과가 원래 벡터와 <strong>“방향”</strong>은 <u>같고</u>, <strong>“배율”</strong>만 상수 λ 만큼만 비례해서 <u>변한다</u>.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727000-43b20905-7c1e-457e-aba0-1ad5aece8ac6.png" alt="스크린샷 2022-01-17 오후 4 36 45" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727034-26906718-5976-475a-88c4-45c51ffb15a6.png" alt="스크린샷 2022-01-17 오후 4 37 01" style="zoom:80%;" /></p>

<blockquote>
  <p>eigenvector의 방향은 똑같고(same direction), 크기만 eigenvalue만큼씩 배수가 됨.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727266-f8a7b2a8-2258-4f5b-af09-cb22033c7c1b.png" alt="스크린샷 2022-01-17 오후 4 38 58" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727382-7c88c62e-3b87-4ad8-ab12-a85539bfc89a.png" alt="스크린샷 2022-01-17 오후 4 39 55" style="zoom:80%;" /></p>

<blockquote>
  <p>고유값과 고유벡터를 구하는 순서는, <strong>먼저 고유값을 구하고나서</strong>, 나중에 Gauss 소거법 사용하여 고유값에 대응하는 고유벡터를 구함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149727691-e03462e3-3025-4faf-b416-c16ac7e30e91.png" alt="스크린샷 2022-01-17 오후 4 42 15" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149727710-fa7fbac0-94dc-43f3-b90b-50467afe0359.png" alt="스크린샷 2022-01-17 오후 4 42 22" style="zoom:80%;" /></p>

<hr />

<p><img src="https://user-images.githubusercontent.com/67947808/149728001-04c1ded9-7189-4e12-9dfc-3fbf15177fd6.png" alt="스크린샷 2022-01-17 오후 4 44 31" style="zoom:80%;" /></p>

<blockquote>
  <p>대각성분을 제외한 모든 성분이 0인 행렬을 대각행렬(diagonal matrix), 적절한 기저변환을 통하여 주어진 행렬을 대각행렬로 변환하는 것을 대각화(diagonolization)이라 함.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/149728224-6ea7600c-b6f5-4378-b748-63fe41491d6d.png" alt="스크린샷 2022-01-17 오후 4 46 17" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149728290-9c6d48f0-cb73-4fea-af30-80ce9e8d5b95.png" alt="스크린샷 2022-01-17 오후 4 46 47" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149728322-120eac70-9b9d-42b4-9322-dbd78b6968b5.png" alt="스크린샷 2022-01-17 오후 4 46 59" style="zoom:80%;" /></p>

<blockquote>
  <p><strong>고유값과 고유벡터를 이용한 n차 정방행렬의 p제곱의 정리를 이용하면 p가 매우 크더라도 연산량을 줄여서 쉽게 p제곱을 구할 수 있음</strong>.</p>
</blockquote>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">출처 주소</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [2주차 - Linear Algebra Review (1)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-Review/" rel="alternate" type="text/html" title="goormNLP [2주차 - Linear Algebra Review (1)]" /><published>2022-01-14T00:00:00+09:00</published><updated>2022-01-14T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20Review</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-Review/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h1 id="linear-algebra-review-1">Linear Algebra Review (1)</h1>

<p><img src="https://user-images.githubusercontent.com/67947808/149459652-2f02c389-827b-416f-b0d7-f3f353c20e8e.png" alt="스크린샷 2022-01-14 오후 2 09 38" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459688-dae5f93d-a8fc-412b-a23c-122bbac286d3.png" alt="스크린샷 2022-01-14 오후 2 09 49" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459719-130c59b2-995b-431f-ab01-b4ea98036fca.png" alt="스크린샷 2022-01-14 오후 2 10 04" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459731-0bae101f-0fba-4cf9-9872-9bc5ab050b21.png" alt="스크린샷 2022-01-14 오후 2 10 31" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459741-2943138d-c16b-4a76-ab67-189b8bc60f69.png" alt="스크린샷 2022-01-14 오후 2 12 13" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459754-bd991613-daca-4433-b86e-e0efd534f295.png" alt="스크린샷 2022-01-14 오후 2 13 37" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459772-a1a34356-83df-43c2-bb99-fb2cdd565040.png" alt="스크린샷 2022-01-14 오후 2 14 36" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459814-acb0e107-3ef5-4909-9187-e14bdfc78784.png" alt="스크린샷 2022-01-14 오후 2 18 39" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459825-9b209f44-7e65-4694-b713-b5c226041fcc.png" alt="스크린샷 2022-01-14 오후 2 19 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459848-7c029bf9-8149-47a9-b906-b89035ca7a63.png" alt="스크린샷 2022-01-14 오후 2 20 29" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460273-79d41b6f-3715-42dc-b068-2f52a9cc033f.png" alt="스크린샷 2022-01-14 오후 2 21 43" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459904-d3869abd-7ca2-4d2d-af33-b7b176e8497e.png" alt="스크린샷 2022-01-14 오후 2 23 39" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459921-477566c1-3070-43ec-8bed-112d76e3e1cc.png" alt="스크린샷 2022-01-14 오후 2 24 31" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459932-19213487-89e9-4514-8292-7a5a47eb79a8.png" alt="스크린샷 2022-01-14 오후 2 25 05" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460314-34c7e740-74e3-4be2-a6e7-f1a2f1d8fa04.png" alt="스크린샷 2022-01-14 오후 2 26 02" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459960-c73d792a-400c-4bf8-b435-01d105278bb3.png" alt="스크린샷 2022-01-14 오후 2 26 40" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459977-be696969-0965-4fa1-a1ad-10be43f13f12.png" alt="스크린샷 2022-01-14 오후 2 27 03" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149459986-1c0d9791-de5b-43b1-aecb-e83f31815bc0.png" alt="스크린샷 2022-01-14 오후 2 27 50" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460016-de5e5e07-04a5-48ea-922c-939e807c4585.png" alt="스크린샷 2022-01-14 오후 2 29 51" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460031-40196820-673c-4480-a369-6df75556a707.png" alt="스크린샷 2022-01-14 오후 2 30 38" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460044-92979705-fe03-4a37-9fc5-d17db938454c.png" alt="스크린샷 2022-01-14 오후 2 31 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460057-a400ff3d-60c3-45e0-ba83-b447f63c347a.png" alt="스크린샷 2022-01-14 오후 2 34 44" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460077-bb0296fd-a214-49cb-9f69-d97837996655.png" alt="스크린샷 2022-01-14 오후 2 35 43" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460092-2ac52ab7-3606-4c86-91fa-0e7748204cef.png" alt="스크린샷 2022-01-14 오후 2 37 28" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460105-f96db737-22dc-4827-ba70-a71a4299c5c7.png" alt="스크린샷 2022-01-14 오후 2 39 02" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460114-9f5e1f5a-2e06-4395-a51b-b98d197154fb.png" alt="스크린샷 2022-01-14 오후 2 43 35" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460122-76f0da2d-015a-4d02-9a1c-ed15f0a7e318.png" alt="스크린샷 2022-01-14 오후 2 45 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460132-d4385732-f429-494f-a57e-533584c99b86.png" alt="스크린샷 2022-01-14 오후 2 50 27" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460143-3c29065d-6b0f-45b9-afe1-b200f8e3bd6f.png" alt="스크린샷 2022-01-14 오후 2 52 03" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460159-911e1ee2-edd8-4a8d-9f06-580e7b618ed5.png" alt="스크린샷 2022-01-14 오후 2 53 55" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460168-0b824cd8-cf43-4fe1-8db8-47a36c264320.png" alt="스크린샷 2022-01-14 오후 2 55 54" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460180-c8c112df-2f03-476e-95c2-ce95d296c906.png" alt="스크린샷 2022-01-14 오후 2 58 13" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460190-3753b70c-b05c-4bc4-a541-121c00b04485.png" alt="스크린샷 2022-01-14 오후 2 59 01" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460205-5c8add80-adf1-407f-9884-436948cf0cf2.png" alt="스크린샷 2022-01-14 오후 2 59 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149460207-f6da936f-706e-434d-9d5b-6f950eac6452.png" alt="스크린샷 2022-01-14 오후 3 00 27" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [2주차 - Linear Algebra (3)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(3)/" rel="alternate" type="text/html" title="goormNLP [2주차 - Linear Algebra (3)]" /><published>2022-01-12T00:00:00+09:00</published><updated>2022-01-12T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(3)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(3)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-5-linear-transformation">Lecture 5: Linear transformation</h2>

<h3 id="transformation">Transformation</h3>

<ul>
  <li>Domain (정의역): Set of all the possible values of <em>x</em>.</li>
  <li>Co-domain (공역): Set of all the possible values of <em>y.</em></li>
  <li>Image: a mapped output <em>y</em>, given <em>x.</em></li>
  <li>Range (치역): Set of all the output values mapped by each <em>x</em> in the domain.</li>
</ul>

<p>=&gt; the output mapped by a particular <em>x</em> is <strong>uniquely determined</strong>.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/149084633-49ff8772-f313-4825-8327-05beb9d66023.png" alt="스크린샷 2022-01-12 오후 4 42 16" style="zoom:85%;" /></p>

<h3 id="linear-transformation">Linear Transformation</h3>

<ul>
  <li>Definition: A transformation (or mapping) <strong>T</strong> is <strong>linear</strong> if:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149084793-c38fde5e-a09f-402e-9f83-604bea3b3431.png" alt="스크린샷 2022-01-12 오후 4 43 32" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149084994-d82a6ff2-96b8-47cd-80da-be55883c4e8d.png" alt="스크린샷 2022-01-12 오후 4 44 56" style="zoom:85%;" /></p>

<ul>
  <li>standard matrix</li>
</ul>

<p>the matrix <strong>A</strong> is called the <strong>standard matrix</strong> of the linear transformation <em>T</em></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149085295-759da393-ab8c-47ee-b47b-5d362eef380a.png" alt="스크린샷 2022-01-12 오후 4 47 01" style="zoom:85%;" /></p>

<h3 id="onto-and-one-to-one">ONTO and ONE-TO-ONE</h3>

<ol>
  <li>ONTO</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/67947808/149085612-8925076d-6bc5-49cb-be48-994500cd2549.png" alt="스크린샷 2022-01-12 오후 4 49 20" style="zoom:85%;" /></p>

<ol>
  <li>ONE-TO-ONE</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/67947808/149085686-c585e831-57ec-4d51-b332-66d1ba7a24c8.png" alt="스크린샷 2022-01-12 오후 4 49 53" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149085930-3dad2ad5-21e2-4dcf-84aa-42c1362ea826.png" alt="스크린샷 2022-01-12 오후 4 51 52" style="zoom:67%;" /></p>

<p>example</p>

<p><img src="https://user-images.githubusercontent.com/67947808/149086046-f5d0dfa1-766f-4394-b2b4-35c779c73c83.png" alt="스크린샷 2022-01-12 오후 4 52 51" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149086120-1d6464a4-5586-4c93-8b8a-2fd8d7d6cca2.png" alt="스크린샷 2022-01-12 오후 4 53 23" style="zoom:85%;" /></p>

<hr />

<h2 id="lecture-6-least-squares">Lecture 6: Least Squares</h2>

<ul>
  <li>The number <strong>u</strong><sup>T</sup><strong>v</strong> is called the <strong>inner product ** or **dot product</strong> of <strong>u</strong> and <strong>v</strong>, and it is written as:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149086562-f62eea94-2685-43e8-a9b0-4b8e5a740bb9.png" alt="스크린샷 2022-01-12 오후 4 56 22" style="zoom:80%;" /></p>

<ul>
  <li>Properties of Inner Product:</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149086682-9838578e-e872-4584-8b2f-7d01cebb2595.png" alt="스크린샷 2022-01-12 오후 4 57 08" style="zoom:80%;" /></p>

<ul>
  <li>Vector Norm (벡터의 길이)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>The <strong>length</strong> (or <strong>norm</strong>) of <strong>v</strong> is the non-negative scalar</td>
      <td> </td>
      <td><strong>v</strong></td>
      <td> </td>
      <td>defined as the square root:</td>
    </tr>
  </tbody>
</table>

<p><img src="https://user-images.githubusercontent.com/67947808/149087030-65871ec0-a52a-4461-8b11-7b6dace23010.png" alt="스크린샷 2022-01-12 오후 4 59 35" style="zoom:80%;" /></p>

<ul>
  <li>Unit vector (단위 벡터)</li>
</ul>

<p>A vector whose <strong>length</strong> is <em>1</em> is called a <strong>unit vector</strong>.</p>

<p>Normalizing a vector: Given a nonzero vector <strong>v</strong>, if we divide it by its length, we obtain a unit vector as:</p>

<p><img src="https://user-images.githubusercontent.com/67947808/149087479-2cde0d21-23e6-424e-8ffa-b97df6cf02c3.png" alt="스크린샷 2022-01-12 오후 5 03 04" style="zoom:80%;" /></p>

<p><strong>u</strong> is in the <u>same direction</u> as <strong>v</strong>, but <u>its length is 1.</u></p>

<ul>
  <li>Distance between Vectors in R<sup>n</sup></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149087744-8bd46f81-3cde-4265-9820-d3902ffd798e.png" alt="스크린샷 2022-01-12 오후 5 04 54" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149087774-14f364ce-8cca-454a-9908-95a25b623ab3.png" alt="스크린샷 2022-01-12 오후 5 05 07" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149087970-562477ed-8225-4165-82c4-1a0060b180ff.png" alt="스크린샷 2022-01-12 오후 5 06 24" /></p>

<ul>
  <li>Inner Product and Angle Between Vectors</li>
</ul>

<p>Inner product between <strong>u</strong> and <strong>v</strong> can be rewritten using their norms and angle:</p>

<p><img src="https://user-images.githubusercontent.com/67947808/149088259-cef58cda-7756-41ee-8b69-37575446fdeb.png" alt="스크린샷 2022-01-12 오후 5 08 24" style="zoom:80%;" /></p>

<ul>
  <li>Orthogonal Vectors</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149088459-02c8b58e-b4f1-4020-afc4-b63f347b7f00.png" alt="스크린샷 2022-01-12 오후 5 09 56" style="zoom:80%;" /></p>

<ul>
  <li><strong>Back to Over-Determined System</strong></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149088711-ab8aef45-c899-4bdd-beeb-70dece19b6cd.png" alt="스크린샷 2022-01-12 오후 5 11 47" style="zoom:80%;" /></p>

<ul>
  <li>Least Squares: Best Approximation Criterion</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149088894-d077863f-2ea1-4118-867a-240782f01dcc.png" alt="스크린샷 2022-01-12 오후 5 13 04" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/149089158-49a147c1-3671-4ecc-908e-14a435adfdd7.png" alt="스크린샷 2022-01-12 오후 5 14 51" style="zoom:80%;" /></p>

<ul>
  <li>Geometric Interpretation of Least Squares.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149089603-c3aa566c-5cc0-4fff-8e84-45be4b78149e.png" alt="스크린샷 2022-01-12 오후 5 17 45" style="zoom:70%;" /></p>

<ul>
  <li>Normal Equation</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/149089942-c80f51f3-2d9f-482f-b7b3-f5813cc88b37.png" alt="스크린샷 2022-01-12 오후 5 20 05" style="zoom:80%;" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry></feed>