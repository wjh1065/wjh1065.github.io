<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-28T12:55:59+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">while True()</title><subtitle>메모장!.</subtitle><author><name>Lee ChangSeok</name></author><entry><title type="html">goormNLP [4주차 - Machine Learning (5)]</title><link href="http://localhost:4000/goormnlp/Machine-Learning-(5)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Machine Learning (5)]" /><published>2022-01-28T00:00:00+09:00</published><updated>2022-01-28T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Machine%20Learning%20(5)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Machine-Learning-(5)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-cross-validation--dimension-reduction">Lecture: Cross Validation &amp; Dimension Reduction</h2>

<p>2022-01-28</p>

<p>지금까지 간단한 데이터를 이용하여 Regression에 대한 실습을 진행하였다.<br />
본 파트는 훈련한 모델의 성능을 높이기 위한 <strong>Cross Validation</strong> 기법과,<br />
<strong>Dimension Reduction</strong>을 통해 특정 Feature를 선택하거나 추출하는 기법에 대한 수업을 진행하였다.</p>

<p>지난 학기 수업때 PCA 기법을 단지 그림과 암기로만 이해를 했었는데,<br />
지난 주에 배운 Linear Algebra 지식을 활용하니 새로운 시점으로 이해를 하게 되었다.</p>

<h2 id="cross-validation">Cross-Validation</h2>

<ul>
  <li>Leave-one-out CV</li>
</ul>

<p>LOOCV는 ex) 총 100개의 데이터가 있으면, 99개의 Train 데이터와 1개의 Test 데이터로 나눈다.<br />
그리고 이러한 행위를 총 100번에 대해서 각 데이터들을 무조건 한 번씩 Test 데이터로 설정한다.<br />
이는 총 100개의 acc 및 loss의 평균을 구하는 방식이다.</p>

<ul>
  <li>K-fold CV
K-fold는 K번씩 데이터를 나누는 것이다.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/151482137-40d3786b-feb7-4b26-ba65-73b2c202f541.png" alt="스크린샷 2022-01-28 오후 12 25 03" style="zoom:67%;" /></p>

<h2 id="dimension-reduction">Dimension Reduction</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151482498-716d83e4-01d2-4420-b7e6-2a2557101dd7.png" alt="스크린샷 2022-01-28 오후 12 29 43" style="zoom:67%;" /></p>

<p><strong>Benefits of Dimension Reduction</strong></p>
<ul>
  <li>Less storage: 데이터의 차원을 줄이니 당연히 용량을 적게 먹음.</li>
  <li>
    <p>Faster computation: 100,000 dim vs. 10 dim vectors 결과 당연히 10개 차원의 계산 속도가 빠름.</p>
  </li>
  <li>
    <p>Noise removal: 더 좋은 performance를 위해 pre-processing를 진행함.</p>
  </li>
  <li><strong>2D / 3D representation</strong>: Interactive visual exploration.</li>
</ul>

<p><strong>Two Main Techniques</strong></p>

<p>==&gt; <em>Feature = Variable = Dimension</em></p>

<ol>
  <li>Feature <strong>selection</strong></li>
</ol>

<p>Selects a subset of the <strong>original variables</strong> as <u>reduced dimensions.</u></p>

<ul>
  <li>Widely-used criteria.</li>
  <li>Typically combinatorial optimization problems.</li>
  <li><strong>greedy methods</strong> are popular.
    <ul>
      <li><u>Forward</u> selection: <strong>Empty</strong> set -&gt; <strong>Add</strong> one variable at a time.</li>
      <li><u>Backward</u> elimination: <strong>Entire</strong> set -&gt; <strong>Remove</strong> one variable at a time.</li>
    </ul>
  </li>
</ul>

<ol>
  <li>Feature <strong>extraction</strong> (<u>MAIN TOPIC</u>)</li>
</ol>

<p>Each reduced dimension <strong>combines</strong> <u>multiple original dimensions.</u></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151483384-daf50fd9-ef0f-42e7-9b9f-e2ea140ffbab.png" alt="스크린샷 2022-01-28 오후 12 41 06" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151483508-dc94ba59-0c30-461c-ba1f-53a5dfe7ac12.png" alt="스크린샷 2022-01-28 오후 12 42 31" style="zoom:67%;" /></p>

<ul>
  <li>
    <p>Represents each <u>reduced dimension</u> as a <strong>linear combination</strong> of original dimensions.</p>
  </li>
  <li>
    <p>Naturally capable of mapping new data to the same space.</p>
  </li>
</ul>

<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>

<ul>
  <li>
    <p>Finds the axis showing the <u>**largest variation**</u>, and <u>**project**</u> all points into this axis.</p>
  </li>
  <li>
    <p>Reduced dimensions are <strong>orthogonal</strong>.</p>
  </li>
  <li>
    <p>Algorithm: Eigen-decomposition.</p>
  </li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/151483999-efe705e8-0bfb-4c69-942d-8621bf12b814.png" alt="스크린샷 2022-01-28 오후 12 48 12" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151484091-7d2aa2bb-d6d9-4a05-a566-3a25279a564a.png" alt="스크린샷 2022-01-28 오후 12 49 21" style="zoom:67%;" /></p>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW14%5D_Multiple_Logistic_Regression.ipynb">HW14 링크</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="machine learning" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Machine Learning (4)]</title><link href="http://localhost:4000/goormnlp/Machine-Learning-(4)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Machine Learning (4)]" /><published>2022-01-27T00:00:00+09:00</published><updated>2022-01-27T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Machine%20Learning%20(4)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Machine-Learning-(4)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-regularization">Lecture: Regularization</h2>

<p>2022-01-27</p>

<p>Regression에 적용할 수 있는 <strong>Regularization</strong> 방법에 대한 이론 및 실습을 진행하였다.</p>

<p>과거 프로젝트를 진행했었을 때 그저 학습이 좋아지게 한다는 도구로써 사용했던 Regularization을 세세하게 이해하였고</p>

<p>다음 프로젝트 및 Competition에 적용하여 더 좋은 점수를 나오게 진행을 해봐야겠다…</p>

<h2 id="the-problem-of-overfitting">The problem of overfitting</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151284807-1d818609-09cb-42b4-a001-7b3ae76fdc7a.png" alt="스크린샷 2022-01-27 오후 12 05 29" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151284842-91bc1e9a-c641-469c-a5c2-99257111a58b.png" alt="스크린샷 2022-01-27 오후 12 05 48" style="zoom:67%;" /></p>

<p>Addressing overfitting:</p>

<ol>
  <li>Reduce number of features.
    <ul>
      <li>Manually select which features to keep.</li>
      <li>Model selection algorithm.</li>
    </ul>
  </li>
  <li><strong>Regularization</strong>
    <ul>
      <li>Keep all the features, but reduce magnitude/values of parameters.</li>
      <li>Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</li>
    </ul>
  </li>
</ol>

<h2 id="cost-function">Cost function</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151285170-7d0442f0-d8d5-4736-8ea8-5f72ba74a925.png" alt="스크린샷 2022-01-27 오후 12 09 32" style="zoom:67%;" /></p>

<p>기존에 있는 cost function에 <strong>regularization parameter</strong>를 추가하고, <strong>λ</strong>의 value를 조정하여 overfitting과 underfitting을 탈출함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151285399-7470729a-6ce7-4bb4-bc6e-548344eb8e21.png" alt="스크린샷 2022-01-27 오후 12 12 03" style="zoom:67%;" /></p>

<p><strong>λ</strong>의 value가 크면 underfit, <strong>λ</strong>의 value가 작으면 overfit이 됨.</p>

<h2 id="practice">Practice</h2>
<p>지난 practice에서는 Linear / Rogistic regression에 대해서 알아보았다.</p>

<p>이번에는 <strong>Polynomial regression</strong>과 <strong>Regularization</strong>을 실습을 통해 알아볼 것이다.</p>

<p><strong>1.</strong> 4차 polynomial function: $y = x^4 + x^3 - 4x^2$ 을 이용해서 데이터를 생성함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151285916-9e2b99a0-b835-4719-a571-e9ef3bafb89c.png" alt="image" /></p>

<p><strong>2.</strong> $d$ 차 곡선을 정의하고 polynomial regression 또한 multiple linear regression 문제와 동일하기 때문에, 아래의 행렬처럼 나타낼 수 있음.</p>

<p>$\hat{y} = w_0 + w_1x + w_2x^2 + … + w_dx^d$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286139-df8803b5-74b5-4314-a627-41792d7a7609.png" alt="스크린샷 2022-01-27 오후 12 20 06" /></p>

<p><strong>3.</strong> 데이터들을 MinMaxScaler하고, 모델 <strong>(3차 항)</strong>을 훈련한 결과, 아래와 같이 잘 훈련된 결과가 plot 됨.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286292-c7b650b9-ac42-4659-870e-a2b104d12a2b.png" alt="image" /></p>

<p><strong>4.</strong> 3차 항의 모델이 아닌 <strong>1차 항과 15차 항 모델</strong>을 훈련한 결과, 아래와 같이 plot 됨.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286520-dd26536f-b9fb-45af-9fdd-518bc0d12b82.png" alt="image" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286727-539b9ee1-4eb6-4ae6-a0e9-82e547a5188e.png" alt="image" style="zoom:67%;" /></p>

<p>1차 항의 결과, <strong>직선</strong>은 너무 간단해서 데이터가 흩뿌려져 있을 때는 그 데이터를 잘 설명하지 못함.<br />
이것은 <strong>underfitting</strong> 되었다고 할 수 있음.</p>

<p>==&gt; <strong>high bias(정답에서 멀음), low variance(한 군데에 모여있음)</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286534-5bb3ea3c-4854-4cb5-a73e-a6cfa57cbc40.png" alt="image" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151286727-539b9ee1-4eb6-4ae6-a0e9-82e547a5188e.png" alt="image" style="zoom:67%;" /></p>

<p>15차 항의 결과, Training data를 모두 커버하려고 매우 복잡한 모델이 되기 때문에, Test data를 잘 못 맞추게 됨.<br />
이것은 <strong>overfitting</strong> 되었다고 할 수 있음.</p>

<p>==&gt; <strong>low bias(정답에서 가까움), high variance(한 군데에 모여있지 않음)</strong></p>

<p><strong>5.</strong> Regularization</p>

<p>regularization term 인 $\lambda\sum_{j=1}^dw_j^2$를 추가하고,. Mean-squared error를 사용한 cost function에 추가하면 다음과 같다..</p>

<p>$L(\mathbf{w}) = \frac{1}{N} \lVert \mathbf{y} - \mathbf{Xw} \rVert^2 + \lambda \sum_{j=1}^d w_j^2$</p>

<p>새롭게 만든 cost function은 <strong>계수들이 작은 값이 되는 것을 선호</strong>하게 됨. 계수들이 커지게 되면 <u>두번째 항이 커지게 돼서 cost 값이 늘어나기 때문</u>.</p>

<p><strong>λ</strong>는 regularization paramter이며 <u>얼마나 큰 제약을 줄 것인가</u>를 결정함. <strong>λ</strong>가 큰 값을 가질수록 계수들을 더 작게 할 수 있음.<br />
이번 practice에서는 <strong>λ</strong>의 값을 1로 설정함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151287326-e49fd01d-dec5-4a77-9bea-47d01027e41e.png" alt="image" style="zoom:67%;" /></p>

<p>Regularization의 결과, 15차 항에서 예측 모델이 진동하는 것이 아닌 부드럽게 학습 데이터들을 지나가는 것을 볼 수 있음.<br />
이렇게 된다면 새롭게 들어오는 데이터도 잘 예측할 수 있게 됨.</p>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW13%5D_Polynomial_Regression_%26_Regularization.ipynb">HW13 링크</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="machine learning" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Machine Learning (3)]</title><link href="http://localhost:4000/goormnlp/Machine-Learning-(3)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Machine Learning (3)]" /><published>2022-01-26T00:00:00+09:00</published><updated>2022-01-26T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Machine%20Learning%20(3)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Machine-Learning-(3)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-logistic-regression">Lecture: Logistic Regression</h2>

<p>2022-01-26</p>

<p>지금까지 주어진 데이터와 가장 잘 맞는 직선을 찾는 <u>Linear Regression</u>을 진행했었다.</p>

<p>이번에는 예측 값이 연속적인 값을 갖지 않는 <strong>Logistic Regression</strong>에 대해서 알아볼 것이다.</p>

<h2 id="classification">Classification</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151110845-ed47db3c-044a-476a-bbd1-a5b7d95ed32c.png" alt="스크린샷 2022-01-26 오후 2 52 41" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/151110968-53e6a463-d30c-4d1e-83a0-2567e3715235.png" alt="스크린샷 2022-01-26 오후 2 54 01" style="zoom: 67%;" /></p>

<h2 id="logistic-function">Logistic function</h2>

<p>Logistic regression을 진행하기 위해서는 출력 값을 0과 1의 값으로 맞춰주어야 한다.</p>

<p>이를 위해서 <strong>logistic function</strong> 을 사용했고, Logistic function은 아래와 같다.</p>

<p>$\sigma(z) = \frac{1}{1 + e^{-z}}$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151111302-e49140c7-b8ab-4fac-8cc1-4b3a4c4bafcf.png" alt="image" /></p>

<p>Logistic regression을 진행할 때 입력 데이터를 $x$, 실제 class 값을 $y$, 예측된 출력 값을 $\hat{y}$라고 하면 $x$는 두가지 변환을 거쳐서 $\hat{y}$가 된다.</p>

<p>$z = wx + b$
$\hat{y} = \sigma(z)$</p>

<p>위에 있는 식의 목표는 $\hat{y}$가 실제 $y$와 가장 가깝게 되도록 하는 $w$와 $b$를 찾는 것 이다.</p>

<h2 id="logistic-loss-function">Logistic loss function</h2>

<p>$\sigma(z) = \frac{1}{1 + e^{-z}}$</p>

<p>$\sigma’(z) = \sigma(z) ( 1 - \sigma(z))$</p>

<p>$\frac{\partial{L}}{\partial{\sigma(z)}} = \frac{(y-\sigma(z))}{\sigma(z)(1-\sigma(z))}$</p>

<p>위와 같은 과정을 통해 구한 cost function $L$은 
$L = -y \log(a) + (y-1)\log(1-a)$이 된다.</p>

<p>만약 $y=1$이라면 $L = -\log(a)$만 남게 되며, 그래프로 표현하면 다음과 같다.<br />
실제 class가 1일때 예측 값이 0에 가까워지면 cost function 값이 커지고, 1에 가까워지면 cost function 값이 작아지는 것을 알 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151111896-942a871a-1abd-4a07-a3e2-7bf81a677637.png" alt="image" /></p>

<p>이제 $y=0$이라면 $L = \log(1-a)$ 만 남게 되며, 그래프로 표현하면 다음과 같다.<br />
예측 값이 실제 값이랑 가까워지면 cost function 값이 작아지고 멀어지면 커지게 됨을 알 수 있다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151112076-8f7cfbef-4506-40fc-ae3b-1967e4fe8f71.png" alt="image" /></p>

<h2 id="practice">Practice</h2>

<p><img src="https://user-images.githubusercontent.com/67947808/151112156-4497c2ab-2a8d-4345-91fd-aebb32f6c82a.png" alt="image" /></p>

<p>빨간색 곡선이 Logistic Regression의 모델이다.<br />
기준값을 정한 후, 그것보다 크면 1, 작으면 0으로 분류를 진행하게 된다.<br />
아래 사진은 기준값을 0.5로 설정한 예시이다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/151112386-0d305db3-0515-477d-a554-776a5db3dca7.png" alt="image" /></p>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW12%5D_Logistic_Regression.ipynb">HW12 링크</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="machine learning" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Machine Learning (2)]</title><link href="http://localhost:4000/goormnlp/Machine-Learning-(2)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Machine Learning (2)]" /><published>2022-01-25T00:00:00+09:00</published><updated>2022-01-25T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Machine%20Learning%20(2)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Machine-Learning-(2)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-linear-regression-with-multiple-variables">Lecture: Linear regression with multiple variables</h2>

<p>2022-01-25</p>

<p>지난번 포스팅에는 하나의 독립 변수 $x$에 대해서 하나의 종속 변수 $y$ 사이의 관계를 알아보는 simple linear regression에 대해서 실습을 하였다.</p>

<p>이번 포스팅에서는 다양한 입력 변수들을 다루는 <strong>multiple linear regression</strong> 에 대해서 알아볼 예정이다.</p>

<h2 id="multiple-features">Multiple features</h2>

<p>기존 하나의 변수를 가진 simple variable은 아래와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905514-fe4ab955-0ae5-4420-bcb3-c90c2f56afe2.png" alt="스크린샷 2022-01-25 오후 12 27 04" style="zoom:67%;" /></p>

<p>그리고 이번에 진행한 여러개의 변수를 가진 multiple variables은 아래와 같다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905595-3587636e-726b-4841-b661-0de24caa8a42.png" alt="스크린샷 2022-01-25 오후 12 27 54" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905641-6d97a16b-2500-4b5f-b31b-bfeb041d7846.png" alt="스크린샷 2022-01-25 오후 12 28 23" style="zoom:67%;" /></p>

<p>스케일링 되지않은 데이터를 이용하여 Regression에 적용한다면 loss가 발산이 될 것이다.</p>

<p>그러므로 모델에 적용 전 데이터들을 <strong>Feature Scaling</strong>을 통해 비슷한 스케일로 조정을 해주어야 한다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150905846-2c2f4cc9-5608-43de-a68c-c4dfef77aab3.png" alt="스크린샷 2022-01-25 오후 12 30 14" style="zoom: 50%;" /></p>

<p>Feature Scaling은 $0&lt;=X&lt;=1$, $-1&lt;=X&lt;=1$과 같은 범위로 나오게 되며,</p>

<p>데이터들의 MIN, MAX를 이용한 방법과 데이터들의 MEAN, STD를 이용한 Normalization이 있음.</p>

<p><strong>Polynomial Regression</strong>은 항이 여러 개인 가설 함수로 결과를 예측하는 방법이다.<br />
<u>서로 다른 두 Feature를 하나의 Feature</u>로 만들어서 해당 Feature를 입력으로 2차 이상의 함수를 예측하는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150906733-e65649a3-d975-47a3-b77c-6fe7a7d455de.png" alt="스크린샷 2022-01-25 오후 12 40 06" style="zoom: 50%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150906938-6179f1fc-5d5e-43e5-a27b-c8e9cda2c6d2.png" alt="스크린샷 2022-01-25 오후 12 42 20" style="zoom:50%;" /></p>

<h2 id="practice">Practice</h2>

<p>실제로 <u>예측을 하고자 할 때 보통 하나 이상의 변수</u>들을 고려해야 한다.<br />
예를 들면 집 값을 예측을 하고자 한다면 <u>집의 크기, 주변의 편의 시설, 위치, 화장실의 개수, 건축 년도 등등 고려해야할 변수들</u>이 많다.</p>

<p>이번 포스팅에서는 다양한 입력 변수 들을 다루는 Multiple linear regression <strong>예시</strong>를 들어 볼 것이다.<br />
사용한 데이터셋: <strong>자동차의 여러 기술적인 사양들을 고려하여 연비를 예측하는 auto miles per gallon(MPG) dataset.</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904247-5cddc436-f796-4068-afc8-93ba00b9068f.png" alt="스크린샷 2022-01-25 오후 12 14 08" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904351-8135aeb4-db27-4afa-8fe3-1b128d58dffa.png" alt="Unknown" /></p>

<p>Accerlation, model_year =&gt; 양의 상관관계.<br />
나머지 =&gt; 음의 상관관계.<br />
위와 같은 상관관계를 통해서 Linear model이 연비를 예측하는데 충분함.</p>

<p>우리가 찾고자하는 모델: \(\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_dx_d\).</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150904749-02fd2e70-59d5-4b03-9678-5c3abb197007.png" alt="스크린샷 2022-01-25 오후 12 18 48" /></p>

<p>맨 앞에 $x_0 = 1$을 추가했었기 때문에 $\mathbf{X}$의 맨 왼쪽 행렬은 1로 이루어져 있음.</p>

<p>cost function: $L(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2$</p>

<p>gradient descent를 이용하여 weights를 찾은 결과 loss가 무한대를 넘어가서 오류가 발생함.</p>

<p>=&gt; 입력 변수들 중에 특정 값들이 너무 커서 일어난 결과임. <strong>Feature scaling</strong>을 통해 데이터 전처리를 진행하면 해결됨.</p>

<p>$x’ = \frac{x - \min(x)}{\max(x)-\min(x)}$</p>

<hr />

<p>만든 모델이 얼마나 정확한지 알아보기위해, Regression 문제에서 주로 사용되는 두개의 기본 지표를 사용함.</p>

<ol>
  <li>Mean absolute error(MAE)</li>
</ol>

<p>${MAE}(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{N} \sum_{i=1}^N \left\vert y^{(i)} - \hat{y}^{(i)}\right\vert$</p>

<ol>
  <li>Root mean squared error(RMSE)</li>
</ol>

<p>${RMSE}(\mathbf{y}, \hat{\mathbf{y}}) =\sqrt{ \frac{1}{N} \sum_{i=1}^N (y^{(i)} - \hat{y}^{(i)})^2}$</p>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW11%5D_Multiple_Linear_Regression.ipynb">HW11 링크</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="machine learning" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [4주차 - Machine Learning (1)]</title><link href="http://localhost:4000/goormnlp/Machine-Learning-(1)/" rel="alternate" type="text/html" title="goormNLP [4주차 - Machine Learning (1)]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Machine%20Learning%20(1)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Machine-Learning-(1)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-linear-regression-with-one-variable">Lecture: Linear regression with one variable</h2>

<p>2022-01-24</p>

<p>Linear regression을 <u>one variable일 때의 Cost function, Gradient descent</u>에 대한 이론 및 실습을 진행하였다.</p>

<p>Linear regression는 Supervised Learning(지도 학습)이라는 큰 범위 내에 포함되어 있고, 한개 이상의 독립 변수 $X$와 종속 변수 $Y$의 선형 관계를 모델링하는 방법론이라 한다.</p>

<p>Regression Problem은 학부 및 대학원에서도 많이 다뤄본 주제이기에 다소 어렵지 않은 내용이였다.</p>

<p>Gradient descent는 머신/딥러닝 알고리즘에서 중요한 이론 중 하나이며 면접에서도 자주 물어보는 내용 중 하나이다.</p>

<h2 id="model-representation">Model representation</h2>

<h3 id="linear-regression">Linear regression</h3>

<p>선형 관계를 모델링한다는 것은 1차로 이루어진 직선을 구하는 것임.</p>

<p>우리의 데이터를 가장 잘 설명하는 최적의 직선을 찾아냄으로써 독립 변수와 종속 변수 사이의 관계를 도출해 내는 과정임.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726237-b44ba52c-0abe-453b-8bbb-f76bf4231181.png" alt="스크린샷 2022-01-24 오후 2 19 18" /></p>

<h3 id="cost-function">Cost function</h3>

<p>우리의 데이터를 가장 잘 설명하는 직선은 우리가 직선을 통해 <u>예측한 값이 실제 데이터의 값과 가장 비슷해야 함</u>. 우리의 모델이 예측한 값은 위에서 알 수 있듯 $f(x_i)$임. 그리고 실제 데이터는 $y$ 입니다. 실제 데이터(위 그림에서 빨간 점) 과 직선 사이의 차이를 줄이는 것이 우리의 목적이며 그것을 바탕으로 cost function을 아래와 같이 정의함.</p>

<p><strong>cost function =</strong> $\frac{1}{N}\sum_{i=1}^n (y_i - f(x_i))^2$</p>

<p>EX)</p>

<p>$f(w) = w^2 + 3w -5$</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150726811-83cb008d-9d2c-4259-bd8f-745344312b5c.png" alt="스크린샷 2022-01-24 오후 2 25 56" /></p>

<p>f_prime = $2w+3$ ==&gt; $w$ = $-3/2$</p>

<h3 id="gradient-descent">Gradient Descent</h3>

<p><img src="https://user-images.githubusercontent.com/67947808/150726913-15732821-cd02-4773-9cd8-7daba1903c20.png" alt="스크린샷 2022-01-24 오후 2 27 13" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fpnum</span> <span class="o">=</span> <span class="n">sympy</span><span class="p">.</span><span class="n">lambdify</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">fprime</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">fpnum</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="mf">10.0</span>   <span class="c1"># starting guess for the min
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">fpnum</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">*</span><span class="mf">0.01</span> <span class="c1"># with 0.01 the step size
</span>
<span class="k">print</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">1.4999999806458753</span>
</code></pre></div></div>

<hr />

<p><a href="https://github.com/wjh1065/goormNLP/blob/main/03_Machine_Learning/sol/%5BHW10%5D_Simple_Linear_Regression.ipynb">HW10 링크</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="machine learning" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">Pytorch 논문 구현 [UNet]</title><link href="http://localhost:4000/pytorch/UNet-(1)/" rel="alternate" type="text/html" title="Pytorch 논문 구현 [UNet]" /><published>2022-01-24T00:00:00+09:00</published><updated>2022-01-24T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/UNet%20(1)</id><content type="html" xml:base="http://localhost:4000/pytorch/UNet-(1)/"><![CDATA[<p>Pytorch Zero to All</p>

<h2 id="unet">UNet</h2>

<p>2022-01-24</p>

<p>UNet 모델은 학부생때 딥러닝 주제를 정하고 케라스로 처음으로 구현해본 모델이다. 구현을 한 경험이 있고 논문 또한 정독을 많이 했기에 파이토치로 구현했을때 어려운 점은 없었다. <del>(class 모듈화 공부를 더 해야할 듯..)</del></p>

<p>edit..
edit..</p>

<hr />]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">Pytorch START</title><link href="http://localhost:4000/pytorch/START/" rel="alternate" type="text/html" title="Pytorch START" /><published>2022-01-23T00:00:00+09:00</published><updated>2022-01-23T00:00:00+09:00</updated><id>http://localhost:4000/pytorch/START</id><content type="html" xml:base="http://localhost:4000/pytorch/START/"><![CDATA[<p>Pytorch Zero to All</p>

<p>2022-01-23</p>

<p>케라스는 파이토치, 텐서플로우에 비해 High Level로써 입문자에게 처음 딥러닝 실습에 적합한 언어라 생각한다. 연구실 생활때, 빠른 결과물을 내야했고.. 파이토치, 텐서플로우를 처음부터 배울 시간이 없었기에 케라스를 이용하여 연구에 대한 결과를 만들어 학회에 발표를 한 경험이 있다.</p>

<p>하지만 인공지능 계열로 취업을 준비했을때 많은 회사들이 Tensorflow보다는 Pytorch 스킬을 요구하였고 나 또한 텐서플로우 기반인 케라스를 다뤄본 적이 있어서 이번 기회에 Pytorch로 전향 할 생각이다.</p>

<p>Pytorch는  상대적으로 Low Level이여서 CUDA 가속화를 할 수 있고 내가 원하는 (정적) 코드로 수정 할 수 있다.</p>

<hr />]]></content><author><name>Lee ChangSeok</name></author><category term="Pytorch" /><category term="study" /><category term="python" /><summary type="html"><![CDATA[Pytorch Zero to All]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra Review (2)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-Review-(2)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra Review (2)]" /><published>2022-01-21T00:00:00+09:00</published><updated>2022-01-21T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20Review%20(2)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-Review-(2)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h1 id="linear-algebra-review-2">Linear Algebra Review (2)</h1>

<p><img src="https://user-images.githubusercontent.com/67947808/150474459-80a43975-cb2c-41d2-b183-6a0c6962669b.png" alt="스크린샷 2022-01-21 오후 2 06 21" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474464-58ab093b-f72d-4916-aa13-215f9617cd4c.png" alt="스크린샷 2022-01-21 오후 2 06 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474466-ef25c98b-9ecf-4669-97af-ff2cbf5908a4.png" alt="스크린샷 2022-01-21 오후 2 08 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474470-f49a1b5d-f58c-4f65-b440-61bfa99b9b1e.png" alt="스크린샷 2022-01-21 오후 2 08 53" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474472-3bb90e5a-6dcc-4a39-b469-ea8e2eef959d.png" alt="스크린샷 2022-01-21 오후 2 10 02" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474474-fd077512-dd7c-4257-8f1c-46cacf9a9438.png" alt="스크린샷 2022-01-21 오후 2 10 17" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474477-1f2c9840-0dc9-441a-a0bc-88548b91b3ec.png" alt="스크린샷 2022-01-21 오후 2 11 13" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474479-3c70745e-0fd9-437f-8564-b0b2107d799a.png" alt="스크린샷 2022-01-21 오후 2 13 45" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474481-9d5a1575-6bff-4f9f-acae-c3ee9e801fa7.png" alt="스크린샷 2022-01-21 오후 2 16 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474483-38fcc253-cffd-4dff-a34d-46cec668f103.png" alt="스크린샷 2022-01-21 오후 2 17 35" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474485-85342d9e-6648-4682-9e83-84ded83560c3.png" alt="스크린샷 2022-01-21 오후 2 21 15" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474486-b35c4b60-b70c-46be-a396-f972a8f08b51.png" alt="스크린샷 2022-01-21 오후 2 24 26" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474490-06a92f22-ef13-4b64-a618-d83612148df0.png" alt="스크린샷 2022-01-21 오후 2 24 34" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474491-98e65ea3-e5a6-4a2a-b352-23348ec3a06b.png" alt="스크린샷 2022-01-21 오후 2 25 06" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474493-668580a3-1c34-4ec5-8883-9eaf79592427.png" alt="스크린샷 2022-01-21 오후 2 25 40" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474495-ee7aa11c-17f3-41bf-9e42-840a769ebf14.png" alt="스크린샷 2022-01-21 오후 2 26 08" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474496-8a8a466c-2181-484a-9863-502d1692d00b.png" alt="스크린샷 2022-01-21 오후 2 27 42" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474498-20fc9f53-c5fe-464a-b9f0-b8c1b87ff941.png" alt="스크린샷 2022-01-21 오후 2 29 41" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474499-48cf00dc-63dc-440d-89e8-2619c5acdaaa.png" alt="스크린샷 2022-01-21 오후 2 31 24" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474500-98a06425-229d-4054-9296-04c60c266ad8.png" alt="스크린샷 2022-01-21 오후 2 32 48" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474502-bb2d9fdd-0a1f-4e01-905b-498afa59115d.png" alt="스크린샷 2022-01-21 오후 2 34 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474504-de00aa03-cbb0-4b3f-87f7-9ed5a986a390.png" alt="스크린샷 2022-01-21 오후 2 36 31" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474505-9f2a4164-052d-40d0-be60-94696e9013af.png" alt="스크린샷 2022-01-21 오후 2 38 07" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474507-8b81b22a-29b8-4e1f-ab64-4c04edd0e991.png" alt="스크린샷 2022-01-21 오후 2 39 38" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474509-8b8491b1-4adb-4b08-9bae-b3e28a4c28a5.png" alt="스크린샷 2022-01-21 오후 2 40 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474512-65922a8a-a373-412b-86cf-41e4b79c8dbc.png" alt="스크린샷 2022-01-21 오후 2 41 19" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474515-3139c7a5-0170-4bf7-855d-868476980c54.png" alt="스크린샷 2022-01-21 오후 2 44 52" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474517-5b9c4d22-9c9a-468c-9733-ba121eb263bb.png" alt="스크린샷 2022-01-21 오후 2 48 56" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474518-379fcb7f-1244-47f0-b4cd-a160780c4254.png" alt="스크린샷 2022-01-21 오후 2 49 29" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474522-ed65b6de-b114-4b3f-ad3d-be9445a212c3.png" alt="스크린샷 2022-01-21 오후 2 51 59" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474524-28e59954-147c-4a07-95b8-22f5eb62daa6.png" alt="스크린샷 2022-01-21 오후 2 53 20" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474525-3a291a9f-8e76-43b3-aaaf-6ce1c3fffbe4.png" alt="스크린샷 2022-01-21 오후 2 55 22" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474528-19b3e087-688d-4e96-ae3c-cf2d103dc08e.png" alt="스크린샷 2022-01-21 오후 2 57 16" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150474736-8e212bf7-eead-44ee-901c-db4dbdafcc25.png" alt="스크린샷 2022-01-21 오후 3 03 08" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (7)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(7)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (7)]" /><published>2022-01-20T00:00:00+09:00</published><updated>2022-01-20T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(7)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(7)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-9-singular-value-decomposition">Lecture 9: Singular Value Decomposition</h2>

<p>2022-01-20</p>

<p>지금까지 배웠던 선형대수 개념을 이용하여 EVD(고유값 분해)를 진행하였다.</p>

<p>EVD는 정방행렬에 대해서만 적용이 가능하고 이번에 배운 SVD(특이값 분해)는 직사각행렬에 폭넓게 사용이 가능하다.</p>

<p>SVD는 저번 포스팅에 올린 Spectral decomposition을 이용하면 직사각행렬을 고유값을 기저로하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li>SVD (Singular Value Decomposition)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150263278-db454564-2ae9-4778-b748-7d03808872df.png" alt="스크린샷 2022-01-20 오전 11 49 35" style="zoom:80%;" /></p>

<ul>
  <li>U = [u_1, u_2, … , u_k+1, … , u_m]는 <strong>AA<sup>T</sup></strong>를 고유값분해로 직교대각화하여 얻은 <u>m by m</u> orthogonal matrix이며, 특히 [u_1, u_2, … , u_k]를 <strong>left singular vectors</strong>라고 함.</li>
  <li>V = [v_1, v_2, … , v_k+1, … , v_n]는 <strong>A<sup>T</sup>A</strong>를 고유값분해로 직교대각화하여 얻은 <u>n by n</u> orthogonal matrix이며, 특히 [v_1, v_2, … , v_k]를 <strong>right singular vectors</strong>라고 함.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/67947808/150264809-8f64cf65-9e4e-4cf2-9bb7-cf761077d2ab.png" alt="스크린샷 2022-01-20 오후 12 04 29" style="zoom:100%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" style="zoom:80%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264842-67d0a27d-b77e-4cb2-b18c-d4deba14fc05.png" alt="스크린샷 2022-01-20 오후 12 04 54" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150264962-5f52d2c6-7f6e-4697-9b1c-5d2ed0810732.png" alt="스크린샷 2022-01-20 오후 12 06 15" /></p>

<blockquote>
  <p>결국 SVD를 계산한다는 것은 AA<sup>T</sup>와 A<sup>T</sup>A의 고유벡터와 고유값을 구하는 것이라는 것을 알 수 있음.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/67947808/150265139-e6c5a1f8-a5b5-486d-9de4-088b9ee25beb.png" alt="스크린샷 2022-01-20 오후 12 08 08" style="zoom:85%;" /></p>

<p>* 그림 출처 : https://en.wikipedia.org/wiki/Singular_value_decomposition</p>

<blockquote>
  <p>직교행렬 V<sup>T</sup>에 의해서 원 행렬 M이 회전(방향 전환)을 하게 되며, 시그마에 의해서 크기가 달라졌고(scale 변환), 다시 직교행렬 U에 의해서 V<sup>T</sup>에 의한 회전과는 반대로 회전(방향 전환)을 함.</p>
</blockquote>

<ul>
  <li>고유값분해(EVD)를 통한 대각화의 경우 고유벡터의 방향은 변화가 없고, <strong>크기만 고유값만큼</strong> 변함.</li>
  <li>특이값분해(SVD)는 <strong>U, V<sup>T</sup>에 의해서 M의 방향이 변하고</strong>, <strong>시그마 특이값(singular values)들 만큼의 크기(scale)가 변했음</strong>을 알 수 있음.</li>
</ul>

<p><strong>Reduced Form of SVD</strong></p>

<p>차원 축소할 때는 reduced SVD를 진행함. full SVD 대비 <strong>reduced SVD는 특이값들 중에서 <u>0인 것들을 제외</u></strong>하고 SVD를 함.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265797-4b73708c-d44c-419d-887b-545126860bbf.png" alt="스크린샷 2022-01-20 오후 12 14 55" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265924-bd356ea3-049d-49af-a7bd-34a42bb23b80.png" alt="스크린샷 2022-01-20 오후 12 16 18" style="zoom: 70%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150265933-8c38c001-696f-4364-894f-0fd57400cdec.png" alt="스크린샷 2022-01-20 오후 12 16 23" style="zoom:90%;" /></p>

<p><strong>example of SVD</strong></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266055-820e3329-1c9f-4743-b2a5-4e1d1f570648.png" alt="스크린샷 2022-01-20 오후 12 17 57" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266061-27d21602-ba62-45e2-bc26-120bbd6e3835.png" alt="스크린샷 2022-01-20 오후 12 18 04" style="zoom:85%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266074-ed37966e-e296-419c-82cd-f14259dcc762.png" alt="스크린샷 2022-01-20 오후 12 18 11" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266082-c1dc2e5c-f4c4-464d-b297-cd35e1ba998e.png" alt="스크린샷 2022-01-20 오후 12 18 19" style="zoom:90%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150266122-154b4e7a-8678-46e4-b94e-36cab5770150.png" alt="스크린샷 2022-01-20 오후 12 18 51" style="zoom:85%;" /></p>

<p><strong>본 그림의 출처는 아래와 같음.</strong></p>

<p><a href="https://rfriend.tistory.com/">R 분석과 프로그래밍</a></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry><entry><title type="html">goormNLP [3주차 - Linear Algebra (6)]</title><link href="http://localhost:4000/goormnlp/Linear_Algebra-(6)/" rel="alternate" type="text/html" title="goormNLP [3주차 - Linear Algebra (6)]" /><published>2022-01-19T00:00:00+09:00</published><updated>2022-01-19T00:00:00+09:00</updated><id>http://localhost:4000/goormnlp/Linear_Algebra%20(6)</id><content type="html" xml:base="http://localhost:4000/goormnlp/Linear_Algebra-(6)/"><![CDATA[<p>Auspice by Goorm, Manage by DAVIAN @ KAIST</p>

<h2 id="lecture-8-advanced-eigendecomposition">Lecture 8: Advanced Eigendecomposition</h2>

<p>2022-01-19</p>

<p>저번 수업 내용을 상기해보면 <em>n</em> x <em>n</em> matrix <em>A</em>가 orthogonally diagonalizable하다면 아래와 같은 식이 성립하게 된다는 것을 알게되었다.</p>

<p><img src="https://user-images.githubusercontent.com/67947808/150086061-513299b4-d650-4909-9fe5-0ef438e3a839.png" alt="스크린샷 2022-01-19 오후 4 43 15" style="zoom:70%;" /></p>

<p>또한 EVD(고유값 분해)를 Algebraic / Geometric multiplicity 관점으로 바라보았다.</p>

<p>내일 수업에서 보게될 SVD(특이값 분해)를 배우기 전에 오늘은 <strong>Spectral Decomposition</strong>에 대해 공부를 진행하였다.</p>

<p>SVD는 행렬의 스펙트럼 이론을 임의의 직사각행렬에 대해 일반화한 것으로 볼 수 있다. <strong>스펙트럼 이론</strong>을 이용하면 직교 정사각행렬을 고유값을 기저로 하여 대각행렬로 분해할 수 있다.</p>

<ul>
  <li><strong>Spectral decomposition</strong></li>
</ul>

<p>An <em>n</em> x <em>n</em> symmetric matrix <em>A</em> has the following properties:</p>

<ol>
  <li><em>A</em> has <em>n</em> <strong>real eigenvalues</strong>, counting multiplicities.</li>
  <li>
    <p>The dimension of the eigenspace for each eigen value λ <strong>equals</strong> the multiplicity of λ as a root of the <em>characteristic equation</em>.
 <strong>The dimension of the eigenspace</strong> ==&gt; Geometric multiplicity.
 <strong>The multiplicity of λ</strong>                          ==&gt; Algebraic multiplicity.</p>
  </li>
  <li>The eigenspaces are mutually orthogonal.</li>
  <li><strong>A</strong> is orthogonally diagonalizable.</li>
</ol>

<p><img src="https://user-images.githubusercontent.com/67947808/150087635-792eb8c9-d6b4-479b-99ed-4e2a2f51102f.png" alt="스크린샷 2022-01-19 오후 4 54 24" style="zoom:67%;" /></p>

<p><img src="https://user-images.githubusercontent.com/67947808/150087662-fddc9e8a-823b-40f6-9a85-9472d84ff61e.png" alt="스크린샷 2022-01-19 오후 4 54 37" style="zoom:67%;" /></p>]]></content><author><name>Lee ChangSeok</name></author><category term="goormNLP" /><category term="study" /><category term="math" /><summary type="html"><![CDATA[Auspice by Goorm, Manage by DAVIAN @ KAIST]]></summary></entry></feed>